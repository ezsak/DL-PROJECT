# Team Members:
Saket Jha : 21CS30044 


# Image Captioning and Classification Project

This project involves building an image captioning model and a classifier to distinguish between captions generated by SmolVLM and a custom model. It is divided into three parts: **Part A**, **Part B**, and **Part C**.

## Table of Contents
1. [Project Overview](#project-overview)
2. [Part A - Image Captioning](#part-a---image-captioning)
3. [Part B - Captioning Robustness Evaluation](#part-b---captioning-robustness-evaluation)
4. [Part C - Caption Classification](#part-c---caption-classification)
5. [Dependencies](#dependencies)
6. [Datasets](#datasets)
7. [Usage](#usage)

---

## Project Overview
This project is focused on building and evaluating an image captioning model based on the ViT-GPT2 architecture and evaluating its robustness under occlusion perturbations. In addition, it includes building a classifier using BERT to distinguish between captions generated by SmolVLM and a custom model.

---

## Part A - Image Captioning
- **Objective**: Build a Transformer-based encoder-decoder model for image captioning.
- **Description**: Use a Vision Transformer (ViT) as the image encoder and GPT2 as the text decoder to generate captions.
- **Metrics**: Evaluated using BLEU, METEOR, and ROUGE-L metrics.
- **Key Functions**:
  - Image preprocessing, tokenization, and training of a captioning model.

---

## Part B - Captioning Robustness Evaluation
- **Objective**: Evaluate the robustness of the image captioning models under occlusion perturbations.
- **Description**: Images are randomly occluded with black patches, and captions are generated using both SmolVLM and a custom model. The performance of the models is evaluated at various levels of occlusion.
- **Metrics**: BLEU, METEOR, and ROUGE-L.
- **Key Functions**:
  - Occlusion of test images.
  - Performance evaluation across different perturbation levels.

---

## Part C - Caption Classification
- **Objective**: Build a classifier to distinguish between captions generated by SmolVLM and a custom model.
- **Description**: Using a BERT-based classifier, we distinguish between captions generated by SmolVLM and those generated by the custom model. The classification is based on the perturbation level and caption content.
- **Key Functions**:
  - Dataset preparation, tokenization, and training of a BERT model for binary classification.

---

## Dependencies
### Part A - Image Captioning
- `torch`: PyTorch library for model building and training.
- `transformers`: Hugging Face library for pre-trained models like ViT and GPT-2.
- `evaluate`: For computing evaluation metrics like BLEU, METEOR, ROUGE.
- `pandas`, `numpy`, `PIL`: For data manipulation and image processing.
- `tqdm`: For progress bar during training.

Install requirements for Part A:
```bash
pip install torch transformers evaluate pandas numpy tqdm
```

### Part B - Captioning Robustness Evaluation
- `torch`, `transformers`: As used in Part A.
- `PIL`: For image processing and perturbation.
- `tqdm`: For progress tracking.

Install requirements for Part B:
```bash
pip install torch transformers tqdm Pillow
```

### Part C - Caption Classification
- `torch`: For model training.
- `transformers`: For BERT-based model and tokenizer.
- `sklearn`: For classification metrics and evaluation.
- `pandas`: For data processing.

Install requirements for Part C:
```bash
pip install torch transformers sklearn pandas
```

---

## Datasets
1. **Image Captioning Dataset**: The dataset used for training the captioning models is assumed to have image-caption pairs stored in CSV format, with the following columns:
   - `filename`: The name of the image file.
   - `caption`: The ground truth caption for the image.

2. **Occlusion Evaluation Dataset**: For Part B, the dataset is based on the same image-caption pairs, with additional columns:
   - `perturbation_level`: The level of image occlusion (percentage of image occluded).
   - `model`: The model used to generate the caption (SmolVLM or Custom).
   - `generated_caption`: The generated caption for the image after occlusion.

3. **Caption Classification Dataset**: For Part C, the dataset contains:
   - `original_caption`: The original caption for the image.
   - `generated_caption`: The generated caption for the image.
   - `perturbation_level`: The level of perturbation applied to the image.
   - `model`: The model used to generate the caption (SmolVLM or Custom).

---

## Usage
1. **Training the Captioning Model (Part A)**: 
   Run the script that trains a ViT-GPT2 model for image captioning, followed by evaluation using BLEU, METEOR, and ROUGE-L scores.

2. **Evaluating Captioning Robustness (Part B)**:
   Perform robustness evaluation by applying random occlusions to images and generating captions using the trained models. Results are recorded at different occlusion levels.

3. **Training and Evaluating the Classifier (Part C)**:
   Use a BERT-based classifier to distinguish between captions generated by SmolVLM and a custom model. The classifier is trained and evaluated on a dataset with perturbation levels and captions.

---

## Conclusion
This project combines image captioning, robustness evaluation, and classification to assess the performance of captioning models under perturbations and to distinguish between captions generated by different models.

---
