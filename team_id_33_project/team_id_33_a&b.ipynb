{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"000e9007b9b8415890f7591bf0ccb0b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"007a159cd5eb46669a4ad61de61df012":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea6d9146f8d04c0c82bfee0c6db70a3f","max":1069,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6551621f9fe54d79bcdb0bb6349ecda4","value":1069}},"00921f3c917243018a7827f95c58dbb6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f6425b5273d4a329a9c86630b14ff58","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af4f4066f43d41eba0dc7ef35e20a16b","value":1355256}},"00c2f8dd3ae148459546733a0b005cd9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"02294593f242451da0e0e21dad033b1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0339025854af46e9b61b7b236d1d6ef4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0403eb26a6434e45b5003c08b3dd4d8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05920796ef9046f480d2fac3c460f802":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19e82418dfb2411a914d41fd49d733a4","placeholder":"​","style":"IPY_MODEL_163bd1a1b36c4918b825ef38a385ac8f","value":" 4.49G/4.49G [00:34&lt;00:00, 203MB/s]"}},"068580eb0c7f44118636704d2c848e3c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06e99c372d3c48ddb288313f8778d5d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08288a046190433381fbee25f0372942":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"083db87879f94e75bd61d3a8c0f875eb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0910032ec54e42e490297a2db4d4c5fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb08abee8b7a4d9f9c1b44a6a00f26c5","max":466391,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e40698ad93f549abbf97a59e2cc309fe","value":466391}},"0b22f384f9324313a8e1e08455454d08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_849661fa6cc14c0c88ed4358ae9e8dc8","IPY_MODEL_e93c24b50c3242aca0434e5e76dc253c","IPY_MODEL_c93d8e9fcc354206ae0127b26ea46576"],"layout":"IPY_MODEL_72b1b0f425ab43b38914b6b30a117e7e"}},"0b84917745584f9989c87585d68d0c70":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e8dc66857c64736a7434738a4a7575d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f035134514144979e9536daa6a91211":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b8187f3ee4f4c81a9abf098c297b744","placeholder":"​","style":"IPY_MODEL_ed00853fcd304c09b25a32f06462d93d","value":" 486/486 [00:00&lt;00:00, 17.2kB/s]"}},"0f5395f03fa044198037135db9c0c166":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9e49056b8ca4190b8096e0a351b79fc","placeholder":"​","style":"IPY_MODEL_423bd0b4853643da838a819f8027b236","value":"chat_template.json: 100%"}},"11336563ca5745ddb5d55738a42a6266":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"121874cd72894a648c673022350a37d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"163bd1a1b36c4918b825ef38a385ac8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19e82418dfb2411a914d41fd49d733a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ae9d16e9a204d54a195610a676b32db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b518c029de146ea8bcdde86e7895619":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b74b6dc3ced4856919ce8fea43fd564":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1de3b92a76344f368a0b41805f90ebe9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e817d5f27f8491d90af74019fd4a71a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21118bc9ccff4292855a29517951879f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2798e35498d048f8ab570c26213ded8b","placeholder":"​","style":"IPY_MODEL_08288a046190433381fbee25f0372942","value":"special_tokens_map.json: 100%"}},"21f50809c09f4027b3d1a706ac4c5952":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"234c720649644c9a9ff43217f9be03c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2798e35498d048f8ab570c26213ded8b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d86178ca3b74866b3eeec53c609ac49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2dd0f0ae311c47468f73da7d4f64358d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"308eb2a03a8b4f54b44e7ca9504a1ff2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30c146fcc2124941adb11febe4dcd0e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"334b19f197a14e56b512311a71868f10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c7d955ef6e7447abe4552ab03996fdf","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a5cdade3ca74da58398ea7e0e3ec30d","value":456318}},"339358cc73624cb2839496061969401e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06e99c372d3c48ddb288313f8778d5d0","max":4492630912,"min":0,"orientation":"horizontal","style":"IPY_MODEL_000e9007b9b8415890f7591bf0ccb0b4","value":4492630912}},"33b7d116f2334945b33fcef0588181ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39f347d674ef4de5ad3aef62ebd242ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30c146fcc2124941adb11febe4dcd0e8","placeholder":"​","style":"IPY_MODEL_0b84917745584f9989c87585d68d0c70","value":" 92.0/92.0 [00:00&lt;00:00, 2.05kB/s]"}},"3a501336189648b1b09f4411f20733f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3aa58cc903cc4e3f82f5414cad2d2018":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b512a4ed8284167ac1a7b3d0ab1b9bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21f50809c09f4027b3d1a706ac4c5952","placeholder":"​","style":"IPY_MODEL_8d9ef42c29644ad9809ff9f33204d2d4","value":"merges.txt: 100%"}},"3df3c8e1dc7f473581bf537c56217672":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc6c34b1968c4d6a9656a28ff81aabcf","max":3523234,"min":0,"orientation":"horizontal","style":"IPY_MODEL_825c80b995b24376885809ef6f4ddee8","value":3523234}},"3e242d47d4bd4c62b425a431691230b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_876c3b996a834312a100cefb4e5ea082","IPY_MODEL_334b19f197a14e56b512311a71868f10","IPY_MODEL_3ec6d3d25092486b9b03e6dbc7a67722"],"layout":"IPY_MODEL_2dd0f0ae311c47468f73da7d4f64358d"}},"3ec6d3d25092486b9b03e6dbc7a67722":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b544d5e3532417f83d7a0014db46a60","placeholder":"​","style":"IPY_MODEL_1b518c029de146ea8bcdde86e7895619","value":" 456k/456k [00:00&lt;00:00, 3.31MB/s]"}},"40f8715ccda54b60b133af9399bebd6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21118bc9ccff4292855a29517951879f","IPY_MODEL_007a159cd5eb46669a4ad61de61df012","IPY_MODEL_db4c37223a524619979f3934721d5c63"],"layout":"IPY_MODEL_92f6b301840c44ec84db5ae58d05d129"}},"411eb38b66a742518ee5e8e0bf7bc046":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8277cf544d0942e09ce760ba9bbd1c11","placeholder":"​","style":"IPY_MODEL_90b44212a4e044c8b4b42c1a3f63414b","value":" 1.04M/1.04M [00:00&lt;00:00, 11.2MB/s]"}},"423bd0b4853643da838a819f8027b236":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48eccdb22d5f4fca9654dcbf09277997":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"496fb22fbf9c4702a3f13c9a9d368e6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8afbdf1a89a1467b8b14eb3bb0844f9b","max":800662,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7773ef9833ab4fe9b12c7e514f82b987","value":800662}},"4a98dd2be27c4b95afafa6075c5395e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b397e86e4af498198ab66e256c8480a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b3dc89287ae42e98006a746cbe1d422":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b6307d3fc8241688834356b24a0a4f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d18b92088984d6c8a0c87317d287d91":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85afde41ab974d548d955c4fbf47fb32","placeholder":"​","style":"IPY_MODEL_fe71d9f1091f498e9e8a62f919ed2ba5","value":"tokenizer_config.json: 100%"}},"4daaafe19c6f452dbc0b890e4fd38dbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"51bba3f501574b73bf9e3d0cfa9ceac8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5426149a59e945d4b12e521f13fc15b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5576579e68bf4f1baa451fe684edfd78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9eb0361212df4345aa417c2d673ea887","max":92,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5b23610e895b474588cb9efbf2680e19","value":92}},"577ea0951e544b249aba6af9627ab8cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"584dc015d0d2417aa18c36046dcc6096":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5962b8fb0b27410fb7e0dc491b17b43e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b23610e895b474588cb9efbf2680e19":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b544d5e3532417f83d7a0014db46a60":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bb5b289a228472baf4508c5d17d0b04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ae9d16e9a204d54a195610a676b32db","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c8042e1dd574441dac82f2f6e57dec8e","value":1042301}},"5bb5e08445c74f3499d4abc8ee645077":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a98dd2be27c4b95afafa6075c5395e8","placeholder":"​","style":"IPY_MODEL_2d86178ca3b74866b3eeec53c609ac49","value":"config.json: 100%"}},"5c7d955ef6e7447abe4552ab03996fdf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d0c605d236e434b9e6314a1b084c364":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae0e496f0b144424a2b0664ccbcf9272","placeholder":"​","style":"IPY_MODEL_121874cd72894a648c673022350a37d7","value":"vocab.json: 100%"}},"5d6843a9cb164136b109f3a3d06cda64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1e23ed3a2bc4ab1beca51a867911384","IPY_MODEL_a2fbf145482a4396961a3f17d07e2759","IPY_MODEL_0f035134514144979e9536daa6a91211"],"layout":"IPY_MODEL_51bba3f501574b73bf9e3d0cfa9ceac8"}},"5d699c57ad2f4789ba86b141d27230a6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6174a0a2b2784e2a95d95e4ce2044566":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63be0a45735e411ba74c642a0c883507":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6551621f9fe54d79bcdb0bb6349ecda4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"680ec5dceb32495dad1ce82bf405cd13":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a50f6d58f1a4fa183829c2819a15f60":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a5cdade3ca74da58398ea7e0e3ec30d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6a9c7ca10ffc425d98b6016fa8255895":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b6307d3fc8241688834356b24a0a4f6","placeholder":"​","style":"IPY_MODEL_c8bdbae54d044ff594f10ffbce91fb9c","value":" 3.52M/3.52M [00:00&lt;00:00, 16.6MB/s]"}},"6ba8573580f04a60bf4762577988c047":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c7f7a90f19f4f0a843182e47ed7ef5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac3140f684c04f30a075ab37309d2592","placeholder":"​","style":"IPY_MODEL_9bc8dd9b2cf94c5591c12702a305354b","value":" 429/429 [00:00&lt;00:00, 9.63kB/s]"}},"6de5baf3a9784e548aea8adf16901738":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0f5395f03fa044198037135db9c0c166","IPY_MODEL_f90f271f4f7f4fcfb1a704539c745fd4","IPY_MODEL_6c7f7a90f19f4f0a843182e47ed7ef5e"],"layout":"IPY_MODEL_068580eb0c7f44118636704d2c848e3c"}},"7146778000064a2e8d024e322ce0c8b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7cf0f995764d494db008a15402e1f7d6","IPY_MODEL_d93c1e3039364b479f4cc0d442057fd9","IPY_MODEL_b362615319c9453e9cdd74ff0215f00d"],"layout":"IPY_MODEL_7c46187645284eb09ad637c0bfd92ef8"}},"72892a9a9d9846edb47d2c8eb0ef87e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e8dc66857c64736a7434738a4a7575d","placeholder":"​","style":"IPY_MODEL_e505ce125c9c41abbd335b7e9ebb56f4","value":" 136/136 [00:00&lt;00:00, 15.2kB/s]"}},"72b1b0f425ab43b38914b6b30a117e7e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7379829071e7492790f7128e05663d9c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76e492d9960945a1a08a49e4eafedde0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7773ef9833ab4fe9b12c7e514f82b987":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77f5cd4716244cbc94152c9ed12cf6ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4d18b92088984d6c8a0c87317d287d91","IPY_MODEL_a4874345a7d9467e9614f4f25e1fe15a","IPY_MODEL_e497c92a61ad498aa94a3e9318a0310e"],"layout":"IPY_MODEL_bf3e7fa2ee6a4d19ba080dea78fca9e3"}},"7b20bf761b9c468fbc3e6d50e4d71802":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_941be16ec5654c5ab94b38e0b1f7d3d4","placeholder":"​","style":"IPY_MODEL_eb7c5d68a3254b8685e375ef8d0b19b5","value":"model.safetensors: 100%"}},"7c46187645284eb09ad637c0bfd92ef8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cf0f995764d494db008a15402e1f7d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccdd3c5ff6234347981a0c2cca9c85c9","placeholder":"​","style":"IPY_MODEL_7fa743bfa9e94ee2a8c38fce4546ddea","value":"config.json: 100%"}},"7d39daa055ff43a59311b1b1f0f8df63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ec5579222784720a2a4ff97a1254811":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5d0c605d236e434b9e6314a1b084c364","IPY_MODEL_496fb22fbf9c4702a3f13c9a9d368e6c","IPY_MODEL_d5e63c0b5c194e58b19d50a148382bb5"],"layout":"IPY_MODEL_4b397e86e4af498198ab66e256c8480a"}},"7f496d96fb444e96adb2f31f4b4f65b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acaa1ca1cc2f4a5abadf1fe4c03dc6cf","placeholder":"​","style":"IPY_MODEL_5962b8fb0b27410fb7e0dc491b17b43e","value":" 1.36M/1.36M [00:00&lt;00:00, 6.96MB/s]"}},"7f737149475b49819186c68874c056c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae03672bfb9b4b4b94bb8adf5531770b","IPY_MODEL_be8a0dcc9210416fbc8477e177d89ed9","IPY_MODEL_72892a9a9d9846edb47d2c8eb0ef87e3"],"layout":"IPY_MODEL_b3f3b08930464a2e80af7840a213d410"}},"7fa743bfa9e94ee2a8c38fce4546ddea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"825c80b995b24376885809ef6f4ddee8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8277cf544d0942e09ce760ba9bbd1c11":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"849661fa6cc14c0c88ed4358ae9e8dc8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfd0feccd2a94437ac62439c6e1aedb1","placeholder":"​","style":"IPY_MODEL_e254411d3c61411c807711aeb1b40876","value":"processor_config.json: 100%"}},"85afde41ab974d548d955c4fbf47fb32":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"876c3b996a834312a100cefb4e5ea082":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_308eb2a03a8b4f54b44e7ca9504a1ff2","placeholder":"​","style":"IPY_MODEL_ade9c982de0c4a68bcbb711ead1fb4fc","value":"merges.txt: 100%"}},"87b89826a4774378aba79fc313bd713b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7279eff6ff04b24ac06c2315c82f8e2","placeholder":"​","style":"IPY_MODEL_8df49525fa90431190a87e8fe8da8120","value":" 466k/466k [00:00&lt;00:00, 6.68MB/s]"}},"8afbdf1a89a1467b8b14eb3bb0844f9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d9ef42c29644ad9809ff9f33204d2d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8df49525fa90431190a87e8fe8da8120":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f6425b5273d4a329a9c86630b14ff58":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"908051ae2bb74342a6ff7c076368f7e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90b44212a4e044c8b4b42c1a3f63414b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90e505bcf5ca4b10ac7fd8fb4071cf46":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"923dd4d6f62c43f8afcdfcc26ff0b8d2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92ec517e636a4063bcec6932fb7d0bd3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92f6b301840c44ec84db5ae58d05d129":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"941be16ec5654c5ab94b38e0b1f7d3d4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94207f8693354df2b4bd12104c9c3ef2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"95dc5d44abb04d20b70a87b2dc60d6c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bdb7a8b7105f42e1968c258ce4ec4c1e","placeholder":"​","style":"IPY_MODEL_48eccdb22d5f4fca9654dcbf09277997","value":"vocab.json: 100%"}},"9b8187f3ee4f4c81a9abf098c297b744":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bc8dd9b2cf94c5591c12702a305354b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9de6634fd81e48e3baa735eb8e75756b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_95dc5d44abb04d20b70a87b2dc60d6c8","IPY_MODEL_5bb5b289a228472baf4508c5d17d0b04","IPY_MODEL_411eb38b66a742518ee5e8e0bf7bc046"],"layout":"IPY_MODEL_a7ec4a8542544040b1ceb6e73033dd64"}},"9eb0361212df4345aa417c2d673ea887":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2fbf145482a4396961a3f17d07e2759":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e224121428214408848cb141615ac83e","max":486,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4daaafe19c6f452dbc0b890e4fd38dbf","value":486}},"a4874345a7d9467e9614f4f25e1fe15a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_908051ae2bb74342a6ff7c076368f7e7","max":4478,"min":0,"orientation":"horizontal","style":"IPY_MODEL_94207f8693354df2b4bd12104c9c3ef2","value":4478}},"a5fa88e6343b469483b5b0aaa52b81f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a73798df977a42d78cc38d8ab6440cf4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e817d5f27f8491d90af74019fd4a71a","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a501336189648b1b09f4411f20733f1","value":26}},"a7a9885dc82f4de0905301a461ac5dc2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7ec4a8542544040b1ceb6e73033dd64":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9e49056b8ca4190b8096e0a351b79fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac3140f684c04f30a075ab37309d2592":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acaa1ca1cc2f4a5abadf1fe4c03dc6cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ade9c982de0c4a68bcbb711ead1fb4fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae03672bfb9b4b4b94bb8adf5531770b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1484729d6cf45e389c907f492ffc7c1","placeholder":"​","style":"IPY_MODEL_0403eb26a6434e45b5003c08b3dd4d8c","value":"generation_config.json: 100%"}},"ae0e496f0b144424a2b0664ccbcf9272":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af4f4066f43d41eba0dc7ef35e20a16b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b0dba0b6c2004a98901b84cc0f24195f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dcddafd19d5a45018274f7a6cfdfad5e","IPY_MODEL_5576579e68bf4f1baa451fe684edfd78","IPY_MODEL_39f347d674ef4de5ad3aef62ebd242ba"],"layout":"IPY_MODEL_76e492d9960945a1a08a49e4eafedde0"}},"b290d9b4cc7e4599abf55e13ee125af3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b512a4ed8284167ac1a7b3d0ab1b9bf","IPY_MODEL_0910032ec54e42e490297a2db4d4c5fc","IPY_MODEL_87b89826a4774378aba79fc313bd713b"],"layout":"IPY_MODEL_5d699c57ad2f4789ba86b141d27230a6"}},"b362615319c9453e9cdd74ff0215f00d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7379829071e7492790f7128e05663d9c","placeholder":"​","style":"IPY_MODEL_a5fa88e6343b469483b5b0aaa52b81f2","value":" 665/665 [00:00&lt;00:00, 16.3kB/s]"}},"b3f3b08930464a2e80af7840a213d410":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b603d31af5e74a09a1aa7c2fd4bd6182":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b94d653ce78e4ab499d541d7bcd3480a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba182928e3d94add9a10fcde99ad9e14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_923dd4d6f62c43f8afcdfcc26ff0b8d2","placeholder":"​","style":"IPY_MODEL_1de3b92a76344f368a0b41805f90ebe9","value":" 7.45k/7.45k [00:00&lt;00:00, 303kB/s]"}},"bbf0bb798ce14036bf23020d9d3e5f06":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ba8573580f04a60bf4762577988c047","placeholder":"​","style":"IPY_MODEL_90e505bcf5ca4b10ac7fd8fb4071cf46","value":" 26.0/26.0 [00:00&lt;00:00, 1.16kB/s]"}},"bc6c34b1968c4d6a9656a28ff81aabcf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc7995d6a6424ff1a717add016bb4110":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bdb7a8b7105f42e1968c258ce4ec4c1e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be8a0dcc9210416fbc8477e177d89ed9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f35351e4ebc44a3fbc003dfe8d756fb0","max":136,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ea0340a9dd984b8ab21d6759207e4a1b","value":136}},"bf185ab065b74ed3b3622a3569794bfd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92ec517e636a4063bcec6932fb7d0bd3","placeholder":"​","style":"IPY_MODEL_11336563ca5745ddb5d55738a42a6266","value":"tokenizer.json: 100%"}},"bf3e7fa2ee6a4d19ba080dea78fca9e3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfd0feccd2a94437ac62439c6e1aedb1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c032d6bb2d1e47b481cfe83b3228f6f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1484729d6cf45e389c907f492ffc7c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1cc4cadea28408cb4350e4609059436":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0339025854af46e9b61b7b236d1d6ef4","placeholder":"​","style":"IPY_MODEL_6a50f6d58f1a4fa183829c2819a15f60","value":"tokenizer.json: 100%"}},"c4c5fd829ef8492ba979e82e55a4d15b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf185ab065b74ed3b3622a3569794bfd","IPY_MODEL_00921f3c917243018a7827f95c58dbb6","IPY_MODEL_7f496d96fb444e96adb2f31f4b4f65b8"],"layout":"IPY_MODEL_a7a9885dc82f4de0905301a461ac5dc2"}},"c58bc882e6ee4bb3a23e4f6c2331d207":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8042e1dd574441dac82f2f6e57dec8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c8bdbae54d044ff594f10ffbce91fb9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c93d8e9fcc354206ae0127b26ea46576":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b74b6dc3ced4856919ce8fea43fd564","placeholder":"​","style":"IPY_MODEL_02294593f242451da0e0e21dad033b1e","value":" 68.0/68.0 [00:00&lt;00:00, 1.21kB/s]"}},"cb08abee8b7a4d9f9c1b44a6a00f26c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb7a94e7e3d941968fb5cc0395218353":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccdd3c5ff6234347981a0c2cca9c85c9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce9e394902704eb7b242527c3ab93380":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5bb5e08445c74f3499d4abc8ee645077","IPY_MODEL_ced5f157decd430d81c47164c3b3928a","IPY_MODEL_ba182928e3d94add9a10fcde99ad9e14"],"layout":"IPY_MODEL_cb7a94e7e3d941968fb5cc0395218353"}},"ced5f157decd430d81c47164c3b3928a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_083db87879f94e75bd61d3a8c0f875eb","max":7446,"min":0,"orientation":"horizontal","style":"IPY_MODEL_33b7d116f2334945b33fcef0588181ba","value":7446}},"d570956b32c646ea9047f1a3046aafdd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_584dc015d0d2417aa18c36046dcc6096","placeholder":"​","style":"IPY_MODEL_234c720649644c9a9ff43217f9be03c8","value":"tokenizer_config.json: 100%"}},"d5e63c0b5c194e58b19d50a148382bb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c58bc882e6ee4bb3a23e4f6c2331d207","placeholder":"​","style":"IPY_MODEL_e79cc295f56a4d0697941aceebf6869b","value":" 801k/801k [00:00&lt;00:00, 19.9MB/s]"}},"d6fd4a8000e04f9bb501ecea98292c51":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d570956b32c646ea9047f1a3046aafdd","IPY_MODEL_a73798df977a42d78cc38d8ab6440cf4","IPY_MODEL_bbf0bb798ce14036bf23020d9d3e5f06"],"layout":"IPY_MODEL_e704990fc41c4ce38f858ac727517eea"}},"d93c1e3039364b479f4cc0d442057fd9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_577ea0951e544b249aba6af9627ab8cb","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b603d31af5e74a09a1aa7c2fd4bd6182","value":665}},"db4c37223a524619979f3934721d5c63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f307bb50fc0848c6a25701f4d2822cde","placeholder":"​","style":"IPY_MODEL_7d39daa055ff43a59311b1b1f0f8df63","value":" 1.07k/1.07k [00:00&lt;00:00, 43.4kB/s]"}},"dcddafd19d5a45018274f7a6cfdfad5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b94d653ce78e4ab499d541d7bcd3480a","placeholder":"​","style":"IPY_MODEL_bc7995d6a6424ff1a717add016bb4110","value":"added_tokens.json: 100%"}},"e1e23ed3a2bc4ab1beca51a867911384":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3aa58cc903cc4e3f82f5414cad2d2018","placeholder":"​","style":"IPY_MODEL_6174a0a2b2784e2a95d95e4ce2044566","value":"preprocessor_config.json: 100%"}},"e224121428214408848cb141615ac83e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e254411d3c61411c807711aeb1b40876":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e291c33d99994c079e799e6a97ec04dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1cc4cadea28408cb4350e4609059436","IPY_MODEL_3df3c8e1dc7f473581bf537c56217672","IPY_MODEL_6a9c7ca10ffc425d98b6016fa8255895"],"layout":"IPY_MODEL_f5e83b43a0e44facb19b470d9e54d395"}},"e40698ad93f549abbf97a59e2cc309fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e497c92a61ad498aa94a3e9318a0310e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b3dc89287ae42e98006a746cbe1d422","placeholder":"​","style":"IPY_MODEL_63be0a45735e411ba74c642a0c883507","value":" 4.48k/4.48k [00:00&lt;00:00, 215kB/s]"}},"e505ce125c9c41abbd335b7e9ebb56f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e704990fc41c4ce38f858ac727517eea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7279eff6ff04b24ac06c2315c82f8e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e79cc295f56a4d0697941aceebf6869b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e93c24b50c3242aca0434e5e76dc253c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9d406a211f04e05a3b408c30be3ee07","max":68,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5426149a59e945d4b12e521f13fc15b5","value":68}},"e9d406a211f04e05a3b408c30be3ee07":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea0340a9dd984b8ab21d6759207e4a1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea6d9146f8d04c0c82bfee0c6db70a3f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb7c5d68a3254b8685e375ef8d0b19b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed00853fcd304c09b25a32f06462d93d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f307bb50fc0848c6a25701f4d2822cde":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f35351e4ebc44a3fbc003dfe8d756fb0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5e83b43a0e44facb19b470d9e54d395":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f90f271f4f7f4fcfb1a704539c745fd4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c032d6bb2d1e47b481cfe83b3228f6f1","max":429,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00c2f8dd3ae148459546733a0b005cd9","value":429}},"fc54d0514178404cab621bbc9af548b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b20bf761b9c468fbc3e6d50e4d71802","IPY_MODEL_339358cc73624cb2839496061969401e","IPY_MODEL_05920796ef9046f480d2fac3c460f802"],"layout":"IPY_MODEL_680ec5dceb32495dad1ce82bf405cd13"}},"fe71d9f1091f498e9e8a62f919ed2ba5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"99167fe8-b448-4025-8fa8-6e9f99334755","cell_type":"markdown","source":"# **Part-A**\n","metadata":{}},{"id":"bb24129c-609c-4e2d-bc99-c92d37ff0d5d","cell_type":"markdown","source":"### Import Required Libraries\n\nIn this cell, we import the necessary libraries for the image captioning model:\n\n- **PyTorch**: for model building and training (`torch`, `torch.nn`, `torch.utils.data`, etc.).\n- **Vision and Image Processing**: libraries like `torchvision`, `albumentations`, and `PIL` for handling image data and transformations.\n- **Transformers**: for utilizing pre-trained Vision Transformer (ViT) and GPT-2 models from the Hugging Face `transformers` library.\n- **Data Handling**: `pandas` for dataset manipulation, `numpy` for numerical computations.\n- **Evaluation**: libraries like `nltk` and `rouge_score` for evaluating captioning performance using BLEU, ROUGE, and METEOR scores.\n- **Other utilities**: `timm` for model creation, `Path` for file paths, and `gc` for garbage collection.\n\nWe also download the NLTK wordnet resource to assist with tokenization and other NLP tasks.\n\nFinally, we set the device to use a GPU if available, otherwise fallback to CPU.\n","metadata":{}},{"id":"b63e430f-b540-4056-aac6-4bee38ba8ee9","cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom timm import create_model, list_models\nfrom types import SimpleNamespace\nfrom tqdm import tqdm\nfrom transformers import ViTModel, GPT2LMHeadModel,GPT2TokenizerFast, get_linear_schedule_with_warmup\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom torch.cuda.amp import GradScaler, autocast\nimport gc\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.meteor_score import meteor_score\n\nimport nltk\nnltk.download('wordnet')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0af73248-0551-4eac-92d4-8f7805cf84fd","cell_type":"markdown","source":"### Download and Extract Dataset\n\nIn this cell, we use the `gdown` library to download a dataset from Google Drive. The provided URL points to a zipped dataset file. We specify the output file name as `custom_captions_dataset.zip`.\n\nOnce the dataset is downloaded, we use Python's `zipfile` module to extract the contents of the zip file into a directory called `custom_captions_dataset`. This step is essential for accessing the dataset for further processing and model training.\n","metadata":{}},{"id":"63dd6ce5-ae77-47ed-a4e8-896bc86fb203","cell_type":"code","source":"import gdown\n\n# Your Google Drive file link\nurl = \"https://drive.google.com/uc?id=1FMVcFM78XZE1KE1rIkGBpCdcdI58S1LB\"\noutput = \"custom_captions_dataset.zip\"\n\ngdown.download(url, output, quiet=False)\n\nimport zipfile\n\nwith zipfile.ZipFile(\"custom_captions_dataset.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"custom_captions_dataset\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cfdb700b-f4d3-45d6-ade4-abde3f2d996c","cell_type":"markdown","source":"### Data Preprocessing and Custom Dataset Class\n\nIn this cell, we define the data augmentation and transformation pipelines, as well as a custom dataset class for loading and preparing data for training:\n\n1. **Data Augmentation**:\n   - `sample_tfms`: A list of augmentation transformations applied to the training images. These include horizontal flipping, random brightness and contrast adjustments, color jittering, random shifting, scaling, and rotating, and hue-saturation adjustments.\n   - `train_tfms`: Composes the transformations for training data, including resizing, normalization, and the augmentations from `sample_tfms`.\n   - `valid_tfms`: Composes transformations for validation data, focusing on resizing, normalization, and ensuring the image is in tensor format.\n\n2. **Custom Dataset Class (`CustomCaptionDataset`)**:\n   - The dataset class inherits from `torch.utils.data.Dataset`. It loads the captions and images from a CSV file and a directory respectively.\n   - The `__getitem__` method fetches an image and its corresponding caption, applies the transformations, and tokenizes the caption.\n   - Captions are tokenized using the provided tokenizer, and the input IDs are shifted left to create the labels.\n   - The `collate_fn` function ensures that the batched images are stacked correctly, and the input IDs and labels are padded and masked appropriately for training.\n\nThis class will be used for both training and validation datasets.\n","metadata":{}},{"id":"a9cb191c-8d69-4c82-9e71-4380a8962a90","cell_type":"code","source":"import os\n\nsample_tfms = [\n    A.HorizontalFlip(),\n    A.RandomBrightnessContrast(),\n    A.ColorJitter(),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.3, rotate_limit=45, p=0.5),\n    A.HueSaturationValue(p=0.3),\n]\n\ntrain_tfms = A.Compose([\n    *sample_tfms,\n    A.Resize(224, 224),\n    A.Normalize(mean=[0.5]*3, std=[0.5]*3),\n    ToTensorV2()\n])\n\nvalid_tfms = A.Compose([\n    A.Resize(224, 224),\n    A.Normalize(mean=[0.5]*3, std=[0.5]*3),\n    ToTensorV2()\n])\n\n\nclass CustomCaptionDataset(Dataset):\n    def __init__(self, csv_path, image_dir, tokenizer, transform):\n        self.df = pd.read_csv(csv_path)\n        self.image_dir = image_dir\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = os.path.join(self.image_dir, row['filename'])\n        caption = row['caption'] + \" <|endoftext|>\"\n\n        image = np.array(Image.open(image_path).convert('RGB'))\n        image = self.transform(image=image)['image']\n\n        # Tokenize caption\n        input_ids = self.tokenizer(caption, truncation=True)['input_ids']\n        labels = input_ids.copy()\n        labels[:-1] = input_ids[1:]  # Shift left\n\n        return image, input_ids, labels\n    \ndef collate_fn(batch):\n    images = [b[0] for b in batch]\n    input_ids = [b[1] for b in batch]\n    labels = [b[2] for b in batch]\n\n    images = torch.stack(images)\n\n    # Pad input_ids and labels\n    input_ids = tokenizer.pad({'input_ids': input_ids}, return_tensors='pt')['input_ids']\n    labels = tokenizer.pad({'input_ids': labels}, return_tensors='pt')['input_ids']\n\n    # Mask for loss\n    mask = (input_ids != tokenizer.pad_token_id)\n    labels[~mask] = -100\n\n    return images, input_ids, labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2c97b6b3-497b-496f-8dc0-d3b5d40661d1","cell_type":"markdown","source":"### Initialize Tokenizer and Load Datasets\n\nIn this cell, we:\n\n1. **Initialize the GPT-2 Tokenizer**:\n   - We load the GPT-2 tokenizer (`GPT2TokenizerFast`) from the Hugging Face model hub.\n   - We set the `pad_token` to be the same as the `eos_token` (end-of-sequence token) because GPT-2 does not have a dedicated padding token by default.\n   - We add a special token `<|endoftext|>` to mark the end of captions in our dataset.\n\n2. **Load Training and Validation Datasets**:\n   - We create instances of the `CustomCaptionDataset` for both the training and validation sets.\n   - We pass the CSV file paths (`train.csv` and `val.csv`) along with the corresponding image directories for both datasets.\n   - The `train_tfms` and `valid_tfms` transformations are applied to the training and validation datasets, respectively.\n   - The tokenizer and transformation pipeline ensure the data is correctly processed before being fed into the model during training and evaluation.\n","metadata":{}},{"id":"c41d6fc9-aa04-4c3d-aa08-8947a15aa861","cell_type":"code","source":"from transformers import GPT2TokenizerFast\n\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_special_tokens({'eos_token': '<|endoftext|>'})\n\nroot_dir = \"custom_captions_dataset/custom_captions_dataset\"\n\ntrain_ds = CustomCaptionDataset(\n    csv_path=os.path.join(root_dir, \"train.csv\"),\n    image_dir=os.path.join(root_dir, \"train\"),\n    tokenizer=tokenizer,\n    transform=train_tfms\n)\n\nval_ds = CustomCaptionDataset(\n    csv_path=os.path.join(root_dir, \"val.csv\"),\n    image_dir=os.path.join(root_dir, \"val\"),\n    tokenizer=tokenizer,\n    transform=valid_tfms\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"73f5f85c-cd09-4088-85a4-6df3e7bb48a9","cell_type":"markdown","source":"### GPT-2 Attention Layer Implementation\n\nIn this cell, we define a custom attention layer inspired by the GPT-2 attention mechanism. This is the core building block for the Transformer model's self-attention mechanism. Key components include:\n\n1. **Initialization (`__init__`)**:\n   - The attention layer receives a configuration object (`config`) that contains hyperparameters such as embedding dimension (`embed_dim`), number of attention heads (`num_heads`), sequence length (`seq_len`), and dropout values.\n   - The embedding dimension must be divisible by the number of attention heads to ensure even splitting of the embeddings across heads.\n   - The `c_attn` linear layer computes the query (`q`), key (`k`), and value (`v`) matrices.\n   - A triangular attention mask (`mask`) is registered to prevent attention to future positions (important for autoregressive models like GPT-2).\n   - `c_proj` is another linear layer to project the output of the attention mechanism back into the embedding space.\n   - Dropout layers are applied for attention (`attn_dropout`) and residual connections (`resid_dropout`).\n\n2. **Forward Pass (`forward`)**:\n   - The input tensor `x` is of shape `(batch_size, seq_len, embed_dim)`.\n   - The `q`, `k`, and `v` matrices are computed by passing the input through the `c_attn` linear layer and splitting it into three parts.\n   - The queries and keys are reshaped to separate the attention heads and are used to calculate attention scores (`qk_t`).\n   - A mask is applied to prevent attending to future tokens in the sequence.\n   - The attention weights are then computed using the softmax function and applied to the value tensor `v`.\n   - The attention output is projected back into the original embedding space using `c_proj`, and residual dropout is applied to the final output.\n\nThis custom attention layer is designed for use in a Transformer-based architecture and mimics the attention mechanism found in the GPT-2 model.\n","metadata":{}},{"id":"1e12d16a","cell_type":"code","source":"class GPT2Attention(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.n_heads = config.num_heads\n        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n        self.head_size = self.embed_dim // self.n_heads\n        self.seq_len = config.seq_len\n        \n        self.c_attn = nn.Linear(self.embed_dim, self.head_size * self.n_heads * 3,bias=True)\n        self.scale = self.head_size ** -0.5\n        \n        self.register_buffer('mask',torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n        \n        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n        \n        self.attn_dropout = nn.Dropout(config.attention_dropout)\n        self.resid_dropout = nn.Dropout(config.residual_dropout)\n        \n        \n    def forward(self, x):\n        b,t,c = x.shape\n        # q,k,v shape individually: batch_size x seq_len x embed_dim\n        # we know that qk_t = q x k_t, where q=bxtxhead_dim, k_t=bxhead_timxt\n        q,k,v = self.c_attn(x).chunk(3,dim=-1)\n        q = q.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3) # batch x n_heads x seq_len x head_dim\n        k = k.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n        v = v.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n        \n        qk_t = (q@k.transpose(-2,-1)) * self.scale\n        qk_t = qk_t.masked_fill(self.mask[:,:,:t,:t]==0,float('-inf'))\n        qk_t = F.softmax(qk_t,dim=-1)\n        weights = self.attn_dropout(qk_t)\n        \n        attention = weights @ v # batch x n_heads x t x head_size\n        attention = attention.permute(0,2,1,3).contiguous().view(b,t,c) # batch x t x embed_dim\n        \n        out = self.c_proj(attention)\n        out = self.resid_dropout(out)\n        \n        return out","metadata":{"execution":{"iopub.execute_input":"2025-04-14T05:27:00.695047Z","iopub.status.busy":"2025-04-14T05:27:00.694753Z","iopub.status.idle":"2025-04-14T05:27:00.703142Z","shell.execute_reply":"2025-04-14T05:27:00.702163Z","shell.execute_reply.started":"2025-04-14T05:27:00.695027Z"},"trusted":true},"outputs":[],"execution_count":6},{"id":"dbaf0ba6-0c5c-4899-9a69-66773f672226","cell_type":"markdown","source":"### GPT-2 Cross Attention Layer Implementation\n\nIn this cell, we define a custom **Cross Attention** layer, similar to the attention mechanism used in transformer models, but specifically designed to work across two different input sequences (e.g., image and text). Key components include:\n\n1. **Initialization (`__init__`)**:\n   - The cross-attention layer takes in a configuration object (`config`) with parameters like embedding dimension (`embed_dim`), number of attention heads (`num_heads`), and sequence length (`seq_len`).\n   - The embedding dimension must be divisible by the number of attention heads for proper splitting.\n   - `q`, `k`, and `v` are three linear layers that project the input queries, keys, and values to the appropriate dimensions for attention calculation.\n   - A scaling factor is applied to the attention scores, based on the size of the attention heads (`head_size`).\n   - `c_proj` is a linear layer that projects the output of the attention mechanism back into the embedding space.\n   - Dropout layers are included to apply regularization during attention computation (`attn_dropout`) and residual connections (`resid_dropout`).\n   - A custom weight initialization function (`_init_weights`) is applied to initialize the weights of the linear layers using a normal distribution and set the biases to zero.\n\n2. **Forward Pass (`forward`)**:\n   - The input `q`, `k`, and `v` are the queries, keys, and values that are passed into the layer. These typically represent different sequences (e.g., text queries and image features).\n   - The queries, keys, and values are passed through the respective linear layers to compute the projected representations.\n   - The reshaped queries, keys, and values are used to compute the attention scores (`qk_t`), which are then normalized using softmax.\n   - The attention weights are applied to the values (`v`), and the output is projected back to the original embedding space using `c_proj`.\n   - Finally, residual dropout is applied to the output.\n\nThis custom cross-attention layer can be used to combine information from different modalities (e.g., images and text), making it useful in models that involve multimodal input, such as vision-language transformers.\n","metadata":{}},{"id":"09849764","cell_type":"code","source":"class GPT2CrossAttention(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.n_heads = config.num_heads\n        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n        self.head_size = self.embed_dim // self.n_heads\n        self.seq_len = config.seq_len\n        \n        self.q = nn.Linear(self.embed_dim,self.embed_dim)\n        self.k = nn.Linear(self.embed_dim,self.embed_dim)\n        self.v = nn.Linear(self.embed_dim,self.embed_dim)\n        self.scale = self.head_size ** -0.5\n        \n        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n        \n        self.attn_dropout = nn.Dropout(config.attention_dropout)\n        self.resid_dropout = nn.Dropout(config.residual_dropout)\n        \n        self.apply(self._init_weights)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        \n        \n    def forward(self, q,k,v):\n        b,t,c = q.shape\n        \n        q = self.q(q)\n        k = self.k(k)\n        v = self.v(v)\n        \n        q = q.view(b,q.size(1),self.n_heads,self.head_size).permute(0,2,1,3) # batch x n_heads x seq_len x head_dim\n        k = k.view(b,k.size(1),self.n_heads,self.head_size).permute(0,2,1,3)\n        v = v.view(b,v.size(1),self.n_heads,self.head_size).permute(0,2,1,3)\n        \n        qk_t = (q@k.transpose(-2,-1)) * self.scale\n        qk_t = F.softmax(qk_t,dim=-1)\n        weights = self.attn_dropout(qk_t)\n        \n        attention = weights @ v # batch x n_heads x t x head_size\n        attention = attention.permute(0,2,1,3).contiguous().view(b,t,c) # batch x t x embed_dim\n        \n        out = self.c_proj(attention)\n        out = self.resid_dropout(out)\n        \n        return out","metadata":{"execution":{"iopub.execute_input":"2025-04-14T05:27:00.706049Z","iopub.status.busy":"2025-04-14T05:27:00.705856Z","iopub.status.idle":"2025-04-14T05:27:00.748170Z","shell.execute_reply":"2025-04-14T05:27:00.747632Z","shell.execute_reply.started":"2025-04-14T05:27:00.706034Z"},"trusted":true},"outputs":[],"execution_count":7},{"id":"f13e6481-725d-4c51-80ab-1fef0bb80f0e","cell_type":"markdown","source":"### GPT-2 MLP (Multilayer Perceptron) Layer Implementation\n\nIn this cell, we define a custom **MLP** (Multilayer Perceptron) layer used within the GPT-2 architecture. The MLP layer is applied to the output of the attention layers to project the features into a new space. The key components include:\n\n1. **Initialization (`__init__`)**:\n   - The MLP layer is configured using the `config` object, which provides parameters such as the embedding dimension (`embed_dim`), the MLP ratio (`mlp_ratio`), and the dropout probability (`mlp_dropout`).\n   - `c_fc`: A linear layer that projects the input embedding dimension to a higher dimensional space (scaled by `mlp_ratio`).\n   - `c_proj`: A linear layer that projects the output of the hidden layer back to the original embedding dimension.\n   - `act`: The activation function applied between the two linear layers. In this case, we use the GELU (Gaussian Error Linear Unit) activation function, commonly used in GPT-2.\n   - `dropout`: A dropout layer applied after the final projection to prevent overfitting.\n\n2. **Forward Pass (`forward`)**:\n   - The input `x` passes through the `c_fc` linear layer, followed by the GELU activation (`act`).\n   - It then passes through the `c_proj` linear layer, and dropout is applied to the output to ensure regularization.\n\nThis MLP layer is used to transform the attention outputs and help the model learn higher-level representations, forming a critical part of the feed-forward network in GPT-2.\n","metadata":{}},{"id":"24a42e6f","cell_type":"code","source":"class GPT2MLP(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.mlp_ratio = config.mlp_ratio\n        self.mlp_dropout = config.mlp_dropout\n        \n        self.c_fc = nn.Linear(self.embed_dim,self.embed_dim*self.mlp_ratio)\n        self.c_proj = nn.Linear(self.embed_dim*self.mlp_ratio,self.embed_dim)\n        self.act = nn.GELU()\n        self.dropout = nn.Dropout(self.mlp_dropout)\n        \n    def forward(self,x):\n        x = self.c_fc(x)\n        x = self.act(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.execute_input":"2025-04-14T05:27:00.748911Z","iopub.status.busy":"2025-04-14T05:27:00.748715Z","iopub.status.idle":"2025-04-14T05:27:00.765045Z","shell.execute_reply":"2025-04-14T05:27:00.764516Z","shell.execute_reply.started":"2025-04-14T05:27:00.748890Z"},"trusted":true},"outputs":[],"execution_count":8},{"id":"1d5d2091-4ada-4998-a667-bba2ed19cd58","cell_type":"markdown","source":"### GPT-2 Block (Transformer Block) Implementation\n\nIn this cell, we define a custom **GPT-2 Block**, which is a standard Transformer block containing a series of layers for both self-attention and cross-attention. The block also includes feed-forward operations. Key components of the block include:\n\n1. **Initialization (`__init__`)**:\n   - The `GPT2Block` is initialized with the following layers:\n     - `ln_1`, `ln_2`, `ln_3`: Layer normalization layers that are applied to the input before the attention and MLP layers.\n     - `attn`: The self-attention layer (`GPT2Attention`) that processes the input sequence on its own.\n     - `cross_attn`: The cross-attention layer (`GPT2CrossAttention`), which attends to external (encoder) outputs, enabling the model to process multimodal data (e.g., images and text).\n     - `mlp`: The MLP layer (`GPT2MLP`) that processes the output of the attention layers and applies non-linear transformations.\n\n2. **Forward Pass (`forward`)**:\n   - The input `x` is first passed through the self-attention layer (`attn`), with the result added back to the input (`x + attention_output`).\n   - Then, the cross-attention layer (`cross_attn`) is applied, attending to an external encoder output (`enc_out`), and the result is added back to the current sequence (`x + cross_attention_output`).\n   - Finally, the MLP layer is applied to the output, and the result is again added back to the input (`x + mlp_output`).\n\nThis GPT-2 Block is a crucial building block of the GPT-2 architecture, handling both self-attention and cross-attention, along with feed-forward processing. It supports complex sequence modeling, particularly useful in tasks involving both sequence data and external context (e.g., image and text).\n","metadata":{}},{"id":"c9e9af01","cell_type":"code","source":"class GPT2Block(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.ln_1 = nn.LayerNorm(self.embed_dim)\n        self.attn = GPT2Attention(config)\n        self.ln_2 = nn.LayerNorm(self.embed_dim)\n        self.mlp = GPT2MLP(config)\n        self.ln_3 = nn.LayerNorm(self.embed_dim)\n        self.cross_attn = GPT2CrossAttention(config)\n        \n    def forward(self,x,enc_out):\n        x = x+self.attn(self.ln_1(x))\n        x = x+self.cross_attn(self.ln_2(x),enc_out,enc_out)\n        x = x+self.mlp(self.ln_3(x))\n        return x\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T05:27:00.766040Z","iopub.status.busy":"2025-04-14T05:27:00.765750Z","iopub.status.idle":"2025-04-14T05:27:00.781855Z","shell.execute_reply":"2025-04-14T05:27:00.781201Z","shell.execute_reply.started":"2025-04-14T05:27:00.766019Z"},"trusted":true},"outputs":[],"execution_count":9},{"id":"49704107-c041-42ee-bfea-17469172a7e1","cell_type":"markdown","source":"### Image Captioning Model (ViT-GPT2 Encoder-Decoder Architecture)\n\nIn this cell, we define the **ImageCaptionModel**, which combines a **Vision Transformer (ViT)** encoder with a **GPT-2** decoder to generate captions for images. This architecture is capable of processing image data (via ViT) and text data (via GPT-2), making it suitable for image captioning tasks.\n\n#### Key Components:\n\n1. **Initialization (`__init__`)**:\n   - **ViT Encoder**: The ViT encoder (`vit_small_patch16_224`) is used as the image feature extractor. The input image is divided into patches, and each patch is passed through the ViT encoder.\n     - `patch_embed`: Embedding layer for patches.\n     - `cls_token`: Class token for the sequence.\n     - `pos_embed`: Positional embedding for patches.\n     - `blocks`: The transformer blocks of ViT.\n     - `encoder_to_decoder`: A linear layer that projects the output of the ViT encoder to match the embedding size of the GPT-2 decoder.\n   \n   - **GPT-2 Decoder**:\n     - **Embedding Layer**: `wte` and `wpe` represent the token and positional embeddings for the GPT-2 decoder.\n     - **Transformer Blocks**: The GPT-2 model consists of a series of `GPT2Block`s, which include attention and feed-forward layers.\n     - **Final Layer Norm (`ln_f`)**: Applied to the final output before feeding into the language model head.\n     - **Language Model Head (`lm_head`)**: A linear layer that projects the output embeddings to the vocabulary size.\n\n2. **Positional Embedding** (`_pos_embed` method):\n   - The positional embedding is added to the input image sequence, with the class token (`cls_token`) prepended to the image tokens.\n\n3. **Training Functionality**:\n   - **Freezing Layers**: The `pretrained_layers_trainable` method allows selective freezing or unfreezing of model layers during training.\n   - **Unfreezing GPT-2 Layers**: The `unfreeze_gpt_layers` method enables the unfreezing of GPT-2 layers for fine-tuning.\n\n4. **Model Initialization from Pretrained Weights** (`from_pretrained` class method):\n   - This method loads the model weights from a pretrained GPT-2 model and initializes the ViT components.\n   - Weights for GPT-2 layers are transposed as needed to match the shapes between ViT and GPT-2 layers.\n\n5. **Forward Pass**:\n   - **Image Input**: The image is passed through the ViT encoder (`patch_embed` and `blocks`), then projected to the decoder embedding space.\n   - **Text Input**: The token embeddings (`wte`) are added to the positional embeddings (`wpe`), and the result is passed through the transformer layers.\n   - **Loss Calculation**: If `labels` are provided, the model computes the cross-entropy loss using the `lm_head` layer.\n\n6. **Text Generation** (`generate` method):\n   - The model can generate captions by autoregressively predicting the next token in the sequence until the maximum number of tokens is reached or the `<|endoftext|>` token is generated. The `temperature` parameter controls the randomness of the predictions, and the `deterministic` flag determines whether to use greedy decoding or sampling.\n\nThis architecture enables the generation of descriptive captions for images by leveraging both vision and language models. It is an encoder-decoder style model where the encoder processes the image, and the decoder generates the caption text.\n","metadata":{}},{"id":"984b732e","cell_type":"code","source":"class ImageCaptionModel(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        \n        self.config = config\n        \n        vit = create_model('vit_small_patch16_224',pretrained=True,num_classes=0)\n        self.patch_embed = vit.patch_embed\n        num_patches = self.patch_embed.num_patches\n        \n        self.cls_token = vit.cls_token\n        embed_len = num_patches + vit.num_prefix_tokens\n        self.pos_embed = vit.pos_embed\n        self.pos_drop = nn.Dropout(p=0.)\n        \n        self.blocks = nn.ModuleList([vit.blocks[i] for i in range(config.depth)])\n        #check this\n        self.encoder_to_decoder = nn.Linear(config.vit_embed_dim, config.embed_dim)\n\n        \n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size,config.embed_dim),\n            wpe = nn.Embedding(config.seq_len,config.embed_dim),\n            drop = nn.Dropout(config.emb_dropout),\n            h = nn.ModuleList([GPT2Block(config) for _ in range(config.depth)]),\n            ln_f = nn.LayerNorm(config.embed_dim)\n        ))\n        self.lm_head = nn.Linear(config.embed_dim,config.vocab_size,bias=False)\n        self.transformer.wte.weight = self.lm_head.weight\n        \n    def _pos_embed(self,x):\n        pos_embed = self.pos_embed\n        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n        x = x + pos_embed\n        return self.pos_drop(x)\n    \n    def pretrained_layers_trainable(self,trainable=False):\n        layers = [\n            self.cls_token, self.patch_embed, self.pos_embed, self.blocks,\n            self.transformer.wte, self.transformer.wpe,\n            self.transformer.ln_f, self.lm_head\n        ]\n        gpt_layers = [[\n            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n            self.transformer.h[i].attn,self.transformer.h[i].mlp\n        ] for i in range(self.config.depth)]\n        for l in gpt_layers:\n            layers.extend(l)\n        \n        for layer in layers:\n            if not isinstance(layer,nn.Parameter):\n                for p in layer.parameters():\n                    p.requires_grad = trainable\n            else:\n                layer.requires_grad = trainable\n                \n        total_frozen_params = sum([p.numel() for p in self.parameters() if not p.requires_grad])\n        print(f'{total_frozen_params=}')\n        \n    def unfreeze_gpt_layers(self,):\n        gpt_layers = [[\n            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n            self.transformer.h[i].attn,self.transformer.h[i].mlp\n        ] for i in range(self.config.depth)]\n        flatten = []\n        for l in gpt_layers:\n            flatten.extend(l)\n            \n        for layer in flatten:\n            if not isinstance(layer,nn.Parameter):\n                for p in layer.parameters():\n                    p.requires_grad = True\n            else:\n                layer.requires_grad = True\n        \n    @classmethod    \n    def from_pretrained(self,config):\n        model = ImageCaptionModel(config)\n        sd = model.state_dict()\n        keys = sd.keys()\n        ignore_matches = ['blocks.','cross_attn.','ln_3','cls_token','pos_embed','patch_embed.','.attn.mask']\n        vit_keys = [key for key in keys if any(match in key for match in ignore_matches)]\n        gpt_keys = [key for key in keys if key not in vit_keys]\n        \n        gpt2_small = GPT2LMHeadModel.from_pretrained('gpt2')\n        sd_hf = gpt2_small.state_dict()\n        hf_keys = sd_hf.keys()\n        hf_keys = [k for k in hf_keys if not k.endswith('.attn.masked_bias')]\n        hf_keys = [k for k in hf_keys if not k.endswith('.attn.bias')]\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        \n        for k in hf_keys:\n            if any(match in k for match in ignore_matches):\n                continue\n            if any(k.endswith(w) for w in transposed):\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n            \n        model.load_state_dict(sd)\n        \n        return model\n    \n    def forward(self,image,input_ids,labels=None):\n        \n        image = self.patch_embed(image)\n        image = self._pos_embed(image)\n        #check this out \n        image_proj = self.encoder_to_decoder(image)\n\n\n        \n        token_embeddings = self.transformer.wte(input_ids) # batch x seq_len\n        pos_embs = torch.arange(0,input_ids.size(1)).to(input_ids.device)\n        positional_embeddings = self.transformer.wpe(pos_embs)\n        input_ids = self.transformer.drop(token_embeddings+positional_embeddings)\n        \n        for i in range(self.config.depth):\n            image = self.blocks[i](image)\n            # input_ids = self.transformer.h[i](input_ids, image)\n            #check this out\n            input_ids = self.transformer.h[i](input_ids, image_proj)\n        \n        input_ids = self.transformer.ln_f(input_ids)\n        \n        if labels is not None:\n            lm_logits = self.lm_head(input_ids)\n            loss = F.cross_entropy(lm_logits.view(-1, lm_logits.shape[-1]), labels.view(-1))\n            return loss\n        \n        lm_logits = self.lm_head(input_ids[:,[-1],:])\n        return lm_logits\n    \n    def generate(self,image,sequence,max_tokens=50,temperature=1.0,deterministic=False):\n        for _ in range(max_tokens):\n            out = self(image,sequence)\n            out = out[:,-1,:] / temperature\n            probs = F.softmax(out,dim=-1)\n            if deterministic:\n                next_token = torch.argmax(probs,dim=-1,keepdim=True)\n            else:\n                next_token = torch.multinomial(probs,num_samples=1)\n            sequence = torch.cat([sequence,next_token],dim=1)\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n            \n        return sequence.cpu().flatten()","metadata":{"execution":{"iopub.execute_input":"2025-04-14T05:27:00.782911Z","iopub.status.busy":"2025-04-14T05:27:00.782595Z","iopub.status.idle":"2025-04-14T05:27:00.801968Z","shell.execute_reply":"2025-04-14T05:27:00.801319Z","shell.execute_reply.started":"2025-04-14T05:27:00.782890Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"32258774-faa6-41bf-913a-c84efa643ac9","cell_type":"markdown","source":"### Trainer Class for Training and Evaluation\n\nThe `Trainer` class handles the training and evaluation process for the **ImageCaptionModel**. It integrates the model training, validation, and saving/loading of the best model, as well as the text generation functionality for captioning.\n\n#### Key Components:\n\n1. **Initialization (`__init__`)**:\n   - **Model Initialization**: The model is instantiated using the `ImageCaptionModel.from_pretrained` method, which loads the pretrained weights and sets up the model. Initially, all pretrained layers are frozen.\n   - **Tokenizer**: A GPT-2 tokenizer (`GPT2TokenizerFast`) is used to process input text. The `bos_token` and `pad_token` are explicitly defined for the tokenizer.\n   - **GradScaler**: The `GradScaler` is used for mixed precision training with `autocast` for automatic mixed-precision scaling.\n   - **DataLoaders**: `train_dl` and `val_dl` are the training and validation DataLoader objects.\n   - **Optimizer**: Adam optimizer with a learning rate scheduler (`OneCycleLR`) for dynamic learning rate adjustment during training.\n\n2. **Saving and Loading Models**:\n   - **`save_model`**: Saves the current model state to the specified directory.\n   - **`load_best_model`**: Loads the best saved model based on validation performance.\n\n3. **Training Loop**:\n   - **`train_one_epoch`**: Handles the training process for a single epoch. The loss is computed for each batch, gradients are backpropagated, and the optimizer is updated using mixed precision.\n   - **`valid_one_epoch`**: Evaluates the model on the validation set for one epoch and computes the loss and perplexity.\n   - **`fit`**: Manages the entire training process over multiple epochs. It trains the model, evaluates it, saves the best model based on validation perplexity, and supports layer freezing/unfreezing during training.\n\n4. **Metrics**:\n   - The metrics DataFrame (`self.metrics`) tracks the loss and perplexity for both training and validation during the training process.\n\n5. **Model Cleaning**:\n   - **`clean`**: Frees up GPU memory by running garbage collection and clearing the cache.\n\n6. **Text Generation (for Captioning)**:\n   - **`generate_caption`**: Given an image, the method generates a caption by feeding the image through the model and decoding the generated sequence into text using the tokenizer.\n\n#### Training and Fine-Tuning:\n- **Freezing Layers**: The model layers can be selectively frozen or unfrozen during training to prevent certain parts of the model (such as the GPT-2 decoder) from being updated in the early training phases. The freezing/unfreezing process is controlled by `freeze_epochs_gpt` and `freeze_epochs_all` in the `train_config`.\n  \n#### Example Usage:\n\n```python\n# Initialize Trainer with model configuration, training configuration, and DataLoaders\ntrainer = Trainer(model_config, train_config, dls)\n\n# Train the model\ntrainer.fit()\n\n# Generate captions from an image\ncaption = trainer.generate_caption(image_path)\nprint(caption)\n","metadata":{}},{"id":"90644614","cell_type":"code","source":"class Trainer:\n    def __init__(self,model_config,train_config, dls):\n        \n        self.train_config = train_config\n        self.model_config = model_config\n        self.device = self.train_config.device\n        \n        self.model = ImageCaptionModel.from_pretrained(model_config).to(self.device)\n        self.model.pretrained_layers_trainable(trainable=False)\n        \n        print(f'trainable parameters: {sum([p.numel() for p in self.model.parameters() if p.requires_grad])}')\n        \n        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.bos_token = self.tokenizer.bos_token  # Explicitly define BOS token\n        \n        self.scaler = GradScaler()\n        \n        self.train_dl, self.val_dl = dls\n        \n        total_steps = len(self.train_dl)\n        \n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.train_config.lr / 25.)\n        self.sched = torch.optim.lr_scheduler.OneCycleLR(\n            self.optim,\n            max_lr=self.train_config.lr,\n            epochs=self.train_config.epochs,\n            steps_per_epoch=total_steps\n        )\n        \n#         self.sched = get_linear_schedule_with_warmup(self.optim,num_warmup_steps=0,num_training_steps=total_steps)\n        \n        self.metrics = pd.DataFrame()\n        self.metrics[['train_loss','train_perplexity','val_loss','val_perplexity']] = None\n        \n        self.gen_tfms = A.Compose([\n            A.Resize(224,224),\n            A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],always_apply=True),\n            ToTensorV2()\n        ])\n            \n        \n    def save_model(self,):\n        self.train_config.model_path.mkdir(exist_ok=True)\n        sd = self.model.state_dict()\n        torch.save(sd,self.train_config.model_path/'captioner.pt')\n        \n        \n    def load_best_model(self,):\n        sd = torch.load(self.train_config.model_path/'captioner.pt')\n        self.model.load_state_dict(sd)\n    \n    \n    def train_one_epoch(self,epoch):\n        \n        prog = tqdm(self.train_dl,total=len(self.train_dl))\n        \n        running_loss = 0.\n        \n        for image, input_ids, labels in prog:\n            \n            with autocast():\n                image = image.to(self.device)\n                input_ids = input_ids.to(self.device)\n                labels = labels.to(self.device)\n                \n                loss = self.model(image,input_ids,labels)\n                \n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optim)\n                self.scaler.update()\n                self.sched.step()\n                self.optim.zero_grad(set_to_none=True)\n                \n                running_loss += loss.item()\n                \n                prog.set_description(f'train loss: {loss.item():.3f}')\n                \n            del image, input_ids, labels, loss\n            \n        train_loss = running_loss / len(self.train_dl)\n        train_pxp = np.exp(train_loss)\n        \n        self.metrics.loc[epoch,['train_loss','train_perplexity']] = (train_loss,train_pxp)\n        \n        \n    @torch.no_grad()\n    def valid_one_epoch(self,epoch):\n        \n        prog = tqdm(self.val_dl,total=len(self.val_dl))\n        \n        running_loss = 0.\n        \n        for image, input_ids, labels in prog:\n            \n            with autocast():\n                image = image.to(self.device)\n                input_ids = input_ids.to(self.device)\n                labels = labels.to(self.device)\n                \n                loss = self.model(image,input_ids,labels)\n                running_loss += loss.item()\n                \n                prog.set_description(f'valid loss: {loss.item():.3f}')\n                \n            del image, input_ids, labels, loss\n            \n        val_loss = running_loss / len(self.val_dl)\n        val_pxp = np.exp(val_loss)\n        \n        self.metrics.loc[epoch,['val_loss','val_perplexity']] = (val_loss,val_pxp)\n        \n        return val_pxp\n        \n        \n    def clean(self):\n        gc.collect()\n        torch.cuda.empty_cache()\n       \n    \n    def fit(self,):\n        \n        best_pxp = 1e9\n        best_epoch = -1\n        prog = tqdm(range(self.train_config.epochs))\n        \n        for epoch in prog:\n            \n            if epoch == self.train_config.freeze_epochs_gpt:\n                self.model.unfreeze_gpt_layers()\n                print('unfreezing GPT2 entirely...')\n                \n            if epoch == self.train_config.freeze_epochs_all:\n                self.model.pretrained_layers_trainable(trainable=True)\n            \n            self.model.train()\n            prog.set_description('training')\n            self.train_one_epoch(epoch)\n            self.clean()\n            \n            self.model.eval()\n            prog.set_description('validating')\n            pxp = self.valid_one_epoch(epoch)\n            self.clean()\n            \n            print(self.metrics.tail(1))\n            \n            if pxp < best_pxp:\n                best_pxp = pxp\n                best_epoch = epoch\n                print('saving best model...')\n                self.save_model()\n                \n        return {\n            'best_perplexity': best_pxp,\n            'best_epoch': best_epoch\n        }\n           \n        \n    @torch.no_grad()\n    def generate_caption(self,image,max_tokens=50,temperature=1.0,deterministic=False):\n        \n        self.model.eval()\n        \n        image = Image.open(image).convert('RGB')\n        image = np.array(image)\n        image = self.gen_tfms(image=image)['image']\n        image = image.unsqueeze(0).to(self.device)\n        sequence = torch.ones(1,1).to(device=self.device).long() * self.tokenizer.bos_token_id\n        \n        caption = self.model.generate(\n            image,\n            sequence,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            deterministic=deterministic\n        )\n        caption = self.tokenizer.decode(caption.numpy(),skip_special_tokens=True)\n        \n        return caption","metadata":{"execution":{"iopub.execute_input":"2025-04-14T05:27:00.803137Z","iopub.status.busy":"2025-04-14T05:27:00.802785Z","iopub.status.idle":"2025-04-14T05:27:00.821802Z","shell.execute_reply":"2025-04-14T05:27:00.821255Z","shell.execute_reply.started":"2025-04-14T05:27:00.803120Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"835a7490-5149-420e-ac65-04b3ef5ee6dd","cell_type":"markdown","source":"### Model Configuration (`model_config`)\n\nThis configuration defines the structure and behavior of the `ImageCaptionModel`.\n\n- **vocab_size**: `50,257` - The size of the vocabulary used by the tokenizer.\n- **embed_dim**: `768` - The dimensionality of the embeddings in the model (for both ViT and GPT-2 components).\n- **vit_embed_dim**: `384` - The embedding dimension for the Vision Transformer (ViT).\n- **num_heads**: `12` - Number of attention heads in the multi-head attention mechanism.\n- **seq_len**: `1024` - Maximum sequence length for input token sequences.\n- **depth**: `12` - Number of transformer layers in the GPT-2 decoder.\n- **attention_dropout**: `0.1` - Dropout rate applied to the attention layers.\n- **residual_dropout**: `0.1` - Dropout applied to the residual connections in the transformer model.\n- **mlp_ratio**: `4` - The ratio determining the hidden layer size in the MLP block.\n- **mlp_dropout**: `0.1` - Dropout rate applied to the MLP (feedforward) block.\n- **emb_dropout**: `0.1` - Dropout applied to the embeddings before the transformer layers.\n\n### Training Configuration (`train_config`)\n\nThis configuration defines the training parameters, optimizer settings, and model saving/loading paths.\n\n- **epochs**: `1` - Number of training epochs. Set to 1 to avoid running out of memory during the second epoch.\n- **freeze_epochs_gpt**: `1` - Number of epochs to freeze the GPT layers. After this epoch, the GPT layers will be unfrozen.\n- **freeze_epochs_all**: `2` - Number of epochs to freeze all pretrained layers (including ViT). After this, all layers will be trainable.\n- **lr**: `1e-4` - Learning rate for the Adam optimizer.\n- **device**: `'cuda'` - The device for training (GPU).\n- **model_path**: `Path('captioner')` - Directory where the trained model will be saved.\n- **batch_size**: `32` - Batch size used during training.\n","metadata":{}},{"id":"f20fc83d","cell_type":"code","source":"model_config = SimpleNamespace(\n    vocab_size = 50_257,\n    embed_dim = 768,\n    vit_embed_dim=384,\n    num_heads = 12,\n    seq_len = 1024,\n    depth = 12,\n    attention_dropout = 0.1,\n    residual_dropout = 0.1,\n    mlp_ratio = 4,\n    mlp_dropout = 0.1,\n    emb_dropout = 0.1,\n)\ntrain_config = SimpleNamespace(\n    epochs = 1,#changed this since oom during epoch 2\n    freeze_epochs_gpt = 1,\n    freeze_epochs_all = 2,\n    lr = 1e-4,\n    device = 'cuda',\n    model_path = Path('captioner'),\n    batch_size = 32\n)","metadata":{"execution":{"iopub.execute_input":"2025-04-14T05:27:00.822665Z","iopub.status.busy":"2025-04-14T05:27:00.822437Z","iopub.status.idle":"2025-04-14T05:27:00.841191Z","shell.execute_reply":"2025-04-14T05:27:00.840522Z","shell.execute_reply.started":"2025-04-14T05:27:00.822650Z"},"trusted":true},"outputs":[],"execution_count":12},{"id":"33e7846d-8207-48c5-84bb-dce0f84c278c","cell_type":"markdown","source":"### Create DataLoaders\n\nWe create two DataLoaders for the training and validation datasets:\n\n- **Training DataLoader**:\n    - Loads the `train_ds` dataset.\n    - Batch size is defined by the `train_config.batch_size`.\n    - Shuffling is enabled for training.\n    - Data is pinned to memory for faster access.\n    - Uses 2 worker threads for parallel data loading.\n    - A custom `collate_fn` function is used to handle batching.\n\n- **Validation DataLoader**:\n    - Loads the `val_ds` dataset.\n    - Batch size is defined by the `train_config.batch_size`.\n    - Shuffling is disabled for validation (no need to shuffle).\n    - Data is pinned to memory for faster access.\n    - Uses 2 worker threads for parallel data loading.\n    - A custom `collate_fn` function is used for batching.\n\n### Create Trainer Instance\n\nA `Trainer` instance is created to handle the training loop. The model configuration, training configuration, and both DataLoaders (training and validation) are passed as arguments.\n\n### Start Training\n\nThe training process is initiated by calling the `fit()` method of the `Trainer` class.\n","metadata":{}},{"id":"ec24ab18","cell_type":"code","source":"# Create DataLoaders\ntrain_dl = torch.utils.data.DataLoader(\n    train_ds,\n    batch_size=train_config.batch_size,\n    shuffle=True,\n    pin_memory=True,\n    num_workers=2,\n    persistent_workers=True,\n    collate_fn=collate_fn\n)\n\nval_dl = torch.utils.data.DataLoader(\n    val_ds,\n    batch_size=train_config.batch_size,\n    shuffle=False,\n    pin_memory=True,\n    num_workers=2,\n    persistent_workers=True,\n    collate_fn=collate_fn\n)\n\n# Create Trainer instance\ntrainer = Trainer(model_config, train_config, (train_dl, val_dl))\n\n# Start training\ntrainer.fit()","metadata":{"execution":{"iopub.execute_input":"2025-04-14T05:27:00.841998Z","iopub.status.busy":"2025-04-14T05:27:00.841816Z","iopub.status.idle":"2025-04-14T05:29:20.358087Z","shell.execute_reply":"2025-04-14T05:29:20.357220Z","shell.execute_reply.started":"2025-04-14T05:27:00.841984Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af654f4b02664d02a60040fc076c163e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/88.2M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f199864cda94794899e6065e64c34ef","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ef88f12a2994a7cb170192822d8e881","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["total_frozen_params=146104704\n","trainable parameters: 28662528\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_31/2932555708.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = GradScaler()\n","/tmp/ipykernel_31/2932555708.py:38: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n","  A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],always_apply=True),\n","training:   0%|          | 0/1 [00:00<?, ?it/s]\n","  0%|          | 0/179 [00:00<?, ?it/s]\u001b[AYou're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/tmp/ipykernel_31/2932555708.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","\n","train loss: 8.315:   0%|          | 0/179 [00:02<?, ?it/s]\u001b[A\n","train loss: 8.315:   1%|          | 1/179 [00:02<07:18,  2.46s/it]\u001b[A\n","train loss: 8.415:   1%|          | 1/179 [00:02<07:18,  2.46s/it]\u001b[A\n","train loss: 8.415:   1%|          | 2/179 [00:02<03:50,  1.30s/it]\u001b[A\n","train loss: 8.403:   1%|          | 2/179 [00:03<03:50,  1.30s/it]\u001b[A\n","train loss: 8.403:   2%|▏         | 3/179 [00:03<02:50,  1.03it/s]\u001b[A\n","train loss: 8.336:   2%|▏         | 3/179 [00:04<02:50,  1.03it/s]\u001b[A\n","train loss: 8.336:   2%|▏         | 4/179 [00:04<02:22,  1.23it/s]\u001b[A\n","train loss: 8.407:   2%|▏         | 4/179 [00:04<02:22,  1.23it/s]\u001b[A\n","train loss: 8.407:   3%|▎         | 5/179 [00:04<02:11,  1.33it/s]\u001b[A\n","train loss: 8.356:   3%|▎         | 5/179 [00:05<02:11,  1.33it/s]\u001b[A\n","train loss: 8.356:   3%|▎         | 6/179 [00:05<02:03,  1.40it/s]\u001b[A\n","train loss: 8.444:   3%|▎         | 6/179 [00:06<02:03,  1.40it/s]\u001b[A\n","train loss: 8.444:   4%|▍         | 7/179 [00:06<01:58,  1.46it/s]\u001b[A\n","train loss: 8.386:   4%|▍         | 7/179 [00:06<01:58,  1.46it/s]\u001b[A\n","train loss: 8.386:   4%|▍         | 8/179 [00:06<01:54,  1.49it/s]\u001b[A\n","train loss: 8.430:   4%|▍         | 8/179 [00:07<01:54,  1.49it/s]\u001b[A\n","train loss: 8.430:   5%|▌         | 9/179 [00:07<01:57,  1.45it/s]\u001b[A\n","train loss: 8.383:   5%|▌         | 9/179 [00:07<01:57,  1.45it/s]\u001b[A\n","train loss: 8.383:   6%|▌         | 10/179 [00:07<01:47,  1.57it/s]\u001b[A\n","train loss: 8.212:   6%|▌         | 10/179 [00:08<01:47,  1.57it/s]\u001b[A\n","train loss: 8.212:   6%|▌         | 11/179 [00:08<01:50,  1.52it/s]\u001b[A\n","train loss: 8.325:   6%|▌         | 11/179 [00:09<01:50,  1.52it/s]\u001b[A\n","train loss: 8.325:   7%|▋         | 12/179 [00:09<01:44,  1.61it/s]\u001b[A\n","train loss: 8.411:   7%|▋         | 12/179 [00:09<01:44,  1.61it/s]\u001b[A\n","train loss: 8.411:   7%|▋         | 13/179 [00:09<01:40,  1.65it/s]\u001b[A\n","train loss: 8.217:   7%|▋         | 13/179 [00:10<01:40,  1.65it/s]\u001b[A\n","train loss: 8.217:   8%|▊         | 14/179 [00:10<01:36,  1.71it/s]\u001b[A\n","train loss: 8.434:   8%|▊         | 14/179 [00:11<01:36,  1.71it/s]\u001b[A\n","train loss: 8.434:   8%|▊         | 15/179 [00:11<01:45,  1.56it/s]\u001b[A\n","train loss: 8.306:   8%|▊         | 15/179 [00:11<01:45,  1.56it/s]\u001b[A\n","train loss: 8.306:   9%|▉         | 16/179 [00:11<01:38,  1.65it/s]\u001b[A\n","train loss: 8.324:   9%|▉         | 16/179 [00:12<01:38,  1.65it/s]\u001b[A\n","train loss: 8.324:   9%|▉         | 17/179 [00:12<01:39,  1.63it/s]\u001b[A\n","train loss: 8.265:   9%|▉         | 17/179 [00:12<01:39,  1.63it/s]\u001b[A\n","train loss: 8.265:  10%|█         | 18/179 [00:12<01:36,  1.66it/s]\u001b[A\n","train loss: 8.280:  10%|█         | 18/179 [00:13<01:36,  1.66it/s]\u001b[A\n","train loss: 8.280:  11%|█         | 19/179 [00:13<01:37,  1.65it/s]\u001b[A\n","train loss: 8.167:  11%|█         | 19/179 [00:13<01:37,  1.65it/s]\u001b[A\n","train loss: 8.167:  11%|█         | 20/179 [00:13<01:37,  1.64it/s]\u001b[A\n","train loss: 8.310:  11%|█         | 20/179 [00:14<01:37,  1.64it/s]\u001b[A\n","train loss: 8.310:  12%|█▏        | 21/179 [00:14<01:39,  1.58it/s]\u001b[A\n","train loss: 8.136:  12%|█▏        | 21/179 [00:15<01:39,  1.58it/s]\u001b[A\n","train loss: 8.136:  12%|█▏        | 22/179 [00:15<01:36,  1.63it/s]\u001b[A\n","train loss: 8.174:  12%|█▏        | 22/179 [00:16<01:36,  1.63it/s]\u001b[A\n","train loss: 8.174:  13%|█▎        | 23/179 [00:16<01:53,  1.38it/s]\u001b[A\n","train loss: 8.066:  13%|█▎        | 23/179 [00:16<01:53,  1.38it/s]\u001b[A\n","train loss: 8.066:  13%|█▎        | 24/179 [00:16<01:44,  1.49it/s]\u001b[A\n","train loss: 8.229:  13%|█▎        | 24/179 [00:17<01:44,  1.49it/s]\u001b[A\n","train loss: 8.229:  14%|█▍        | 25/179 [00:17<01:52,  1.37it/s]\u001b[A\n","train loss: 7.961:  14%|█▍        | 25/179 [00:18<01:52,  1.37it/s]\u001b[A\n","train loss: 7.961:  15%|█▍        | 26/179 [00:18<01:41,  1.51it/s]\u001b[A\n","train loss: 8.102:  15%|█▍        | 26/179 [00:18<01:41,  1.51it/s]\u001b[A\n","train loss: 8.102:  15%|█▌        | 27/179 [00:18<01:34,  1.61it/s]\u001b[A\n","train loss: 8.193:  15%|█▌        | 27/179 [00:19<01:34,  1.61it/s]\u001b[A\n","train loss: 8.193:  16%|█▌        | 28/179 [00:19<01:31,  1.65it/s]\u001b[A\n","train loss: 7.929:  16%|█▌        | 28/179 [00:19<01:31,  1.65it/s]\u001b[A\n","train loss: 7.929:  16%|█▌        | 29/179 [00:19<01:29,  1.67it/s]\u001b[A\n","train loss: 8.071:  16%|█▌        | 29/179 [00:20<01:29,  1.67it/s]\u001b[A\n","train loss: 8.071:  17%|█▋        | 30/179 [00:20<01:30,  1.65it/s]\u001b[A\n","train loss: 8.056:  17%|█▋        | 30/179 [00:21<01:30,  1.65it/s]\u001b[A\n","train loss: 8.056:  17%|█▋        | 31/179 [00:21<01:34,  1.57it/s]\u001b[A\n","train loss: 7.921:  17%|█▋        | 31/179 [00:21<01:34,  1.57it/s]\u001b[A\n","train loss: 7.921:  18%|█▊        | 32/179 [00:21<01:39,  1.47it/s]\u001b[A\n","train loss: 7.968:  18%|█▊        | 32/179 [00:22<01:39,  1.47it/s]\u001b[A\n","train loss: 7.968:  18%|█▊        | 33/179 [00:22<01:43,  1.41it/s]\u001b[A\n","train loss: 7.952:  18%|█▊        | 33/179 [00:23<01:43,  1.41it/s]\u001b[A\n","train loss: 7.952:  19%|█▉        | 34/179 [00:23<01:41,  1.42it/s]\u001b[A\n","train loss: 7.956:  19%|█▉        | 34/179 [00:23<01:41,  1.42it/s]\u001b[A\n","train loss: 7.956:  20%|█▉        | 35/179 [00:23<01:34,  1.52it/s]\u001b[A\n","train loss: 8.007:  20%|█▉        | 35/179 [00:24<01:34,  1.52it/s]\u001b[A\n","train loss: 8.007:  20%|██        | 36/179 [00:24<01:36,  1.49it/s]\u001b[A\n","train loss: 7.790:  20%|██        | 36/179 [00:25<01:36,  1.49it/s]\u001b[A\n","train loss: 7.790:  21%|██        | 37/179 [00:25<01:37,  1.46it/s]\u001b[A\n","train loss: 7.653:  21%|██        | 37/179 [00:26<01:37,  1.46it/s]\u001b[A\n","train loss: 7.653:  21%|██        | 38/179 [00:26<01:40,  1.40it/s]\u001b[A\n","train loss: 7.504:  21%|██        | 38/179 [00:26<01:40,  1.40it/s]\u001b[A\n","train loss: 7.504:  22%|██▏       | 39/179 [00:26<01:31,  1.52it/s]\u001b[A\n","train loss: 7.682:  22%|██▏       | 39/179 [00:27<01:31,  1.52it/s]\u001b[A\n","train loss: 7.682:  22%|██▏       | 40/179 [00:27<01:38,  1.41it/s]\u001b[A\n","train loss: 7.492:  22%|██▏       | 40/179 [00:28<01:38,  1.41it/s]\u001b[A\n","train loss: 7.492:  23%|██▎       | 41/179 [00:28<01:35,  1.45it/s]\u001b[A\n","train loss: 7.430:  23%|██▎       | 41/179 [00:28<01:35,  1.45it/s]\u001b[A\n","train loss: 7.430:  23%|██▎       | 42/179 [00:28<01:36,  1.42it/s]\u001b[A\n","train loss: 7.534:  23%|██▎       | 42/179 [00:29<01:36,  1.42it/s]\u001b[A\n","train loss: 7.534:  24%|██▍       | 43/179 [00:29<01:30,  1.51it/s]\u001b[A\n","train loss: 7.416:  24%|██▍       | 43/179 [00:30<01:30,  1.51it/s]\u001b[A\n","train loss: 7.416:  25%|██▍       | 44/179 [00:30<01:32,  1.45it/s]\u001b[A\n","train loss: 7.265:  25%|██▍       | 44/179 [00:30<01:32,  1.45it/s]\u001b[A\n","train loss: 7.265:  25%|██▌       | 45/179 [00:30<01:25,  1.57it/s]\u001b[A\n","train loss: 7.302:  25%|██▌       | 45/179 [00:31<01:25,  1.57it/s]\u001b[A\n","train loss: 7.302:  26%|██▌       | 46/179 [00:31<01:20,  1.65it/s]\u001b[A\n","train loss: 7.165:  26%|██▌       | 46/179 [00:31<01:20,  1.65it/s]\u001b[A\n","train loss: 7.165:  26%|██▋       | 47/179 [00:31<01:21,  1.62it/s]\u001b[A\n","train loss: 7.146:  26%|██▋       | 47/179 [00:32<01:21,  1.62it/s]\u001b[A\n","train loss: 7.146:  27%|██▋       | 48/179 [00:32<01:18,  1.67it/s]\u001b[A\n","train loss: 6.946:  27%|██▋       | 48/179 [00:33<01:18,  1.67it/s]\u001b[A\n","train loss: 6.946:  27%|██▋       | 49/179 [00:33<01:22,  1.58it/s]\u001b[A\n","train loss: 6.919:  27%|██▋       | 49/179 [00:33<01:22,  1.58it/s]\u001b[A\n","train loss: 6.919:  28%|██▊       | 50/179 [00:33<01:17,  1.66it/s]\u001b[A\n","train loss: 6.916:  28%|██▊       | 50/179 [00:34<01:17,  1.66it/s]\u001b[A\n","train loss: 6.916:  28%|██▊       | 51/179 [00:34<01:15,  1.71it/s]\u001b[A\n","train loss: 6.932:  28%|██▊       | 51/179 [00:34<01:15,  1.71it/s]\u001b[A\n","train loss: 6.932:  29%|██▉       | 52/179 [00:34<01:15,  1.68it/s]\u001b[A\n","train loss: 6.714:  29%|██▉       | 52/179 [00:35<01:15,  1.68it/s]\u001b[A\n","train loss: 6.714:  30%|██▉       | 53/179 [00:35<01:17,  1.63it/s]\u001b[A\n","train loss: 6.798:  30%|██▉       | 53/179 [00:36<01:17,  1.63it/s]\u001b[A\n","train loss: 6.798:  30%|███       | 54/179 [00:36<01:18,  1.59it/s]\u001b[A\n","train loss: 6.714:  30%|███       | 54/179 [00:36<01:18,  1.59it/s]\u001b[A\n","train loss: 6.714:  31%|███       | 55/179 [00:36<01:17,  1.59it/s]\u001b[A\n","train loss: 6.444:  31%|███       | 55/179 [00:37<01:17,  1.59it/s]\u001b[A\n","train loss: 6.444:  31%|███▏      | 56/179 [00:37<01:16,  1.61it/s]\u001b[A\n","train loss: 6.325:  31%|███▏      | 56/179 [00:38<01:16,  1.61it/s]\u001b[A\n","train loss: 6.325:  32%|███▏      | 57/179 [00:38<01:16,  1.58it/s]\u001b[A\n","train loss: 6.261:  32%|███▏      | 57/179 [00:38<01:16,  1.58it/s]\u001b[A\n","train loss: 6.261:  32%|███▏      | 58/179 [00:38<01:13,  1.64it/s]\u001b[A\n","train loss: 6.325:  32%|███▏      | 58/179 [00:39<01:13,  1.64it/s]\u001b[A\n","train loss: 6.325:  33%|███▎      | 59/179 [00:39<01:13,  1.63it/s]\u001b[A\n","train loss: 6.556:  33%|███▎      | 59/179 [00:39<01:13,  1.63it/s]\u001b[A\n","train loss: 6.556:  34%|███▎      | 60/179 [00:39<01:11,  1.66it/s]\u001b[A\n","train loss: 6.478:  34%|███▎      | 60/179 [00:40<01:11,  1.66it/s]\u001b[A\n","train loss: 6.478:  34%|███▍      | 61/179 [00:40<01:17,  1.52it/s]\u001b[A\n","train loss: 6.275:  34%|███▍      | 61/179 [00:41<01:17,  1.52it/s]\u001b[A\n","train loss: 6.275:  35%|███▍      | 62/179 [00:41<01:38,  1.19it/s]\u001b[A\n","train loss: 6.302:  35%|███▍      | 62/179 [00:42<01:38,  1.19it/s]\u001b[A\n","train loss: 6.302:  35%|███▌      | 63/179 [00:42<01:31,  1.27it/s]\u001b[A\n","train loss: 6.288:  35%|███▌      | 63/179 [00:43<01:31,  1.27it/s]\u001b[A\n","train loss: 6.288:  36%|███▌      | 64/179 [00:43<01:21,  1.42it/s]\u001b[A\n","train loss: 6.424:  36%|███▌      | 64/179 [00:43<01:21,  1.42it/s]\u001b[A\n","train loss: 6.424:  36%|███▋      | 65/179 [00:43<01:19,  1.44it/s]\u001b[A\n","train loss: 6.522:  36%|███▋      | 65/179 [00:44<01:19,  1.44it/s]\u001b[A\n","train loss: 6.522:  37%|███▋      | 66/179 [00:44<01:21,  1.39it/s]\u001b[A\n","train loss: 6.072:  37%|███▋      | 66/179 [00:45<01:21,  1.39it/s]\u001b[A\n","train loss: 6.072:  37%|███▋      | 67/179 [00:45<01:19,  1.41it/s]\u001b[A\n","train loss: 6.145:  37%|███▋      | 67/179 [00:45<01:19,  1.41it/s]\u001b[A\n","train loss: 6.145:  38%|███▊      | 68/179 [00:45<01:13,  1.51it/s]\u001b[A\n","train loss: 6.097:  38%|███▊      | 68/179 [00:46<01:13,  1.51it/s]\u001b[A\n","train loss: 6.097:  39%|███▊      | 69/179 [00:46<01:16,  1.43it/s]\u001b[A\n","train loss: 6.056:  39%|███▊      | 69/179 [00:47<01:16,  1.43it/s]\u001b[A\n","train loss: 6.056:  39%|███▉      | 70/179 [00:47<01:15,  1.45it/s]\u001b[A\n","train loss: 6.164:  39%|███▉      | 70/179 [00:47<01:15,  1.45it/s]\u001b[A\n","train loss: 6.164:  40%|███▉      | 71/179 [00:47<01:12,  1.49it/s]\u001b[A\n","train loss: 6.510:  40%|███▉      | 71/179 [00:48<01:12,  1.49it/s]\u001b[A\n","train loss: 6.510:  40%|████      | 72/179 [00:48<01:12,  1.47it/s]\u001b[A\n","train loss: 5.946:  40%|████      | 72/179 [00:49<01:12,  1.47it/s]\u001b[A\n","train loss: 5.946:  41%|████      | 73/179 [00:49<01:10,  1.50it/s]\u001b[A\n","train loss: 6.130:  41%|████      | 73/179 [00:49<01:10,  1.50it/s]\u001b[A\n","train loss: 6.130:  41%|████▏     | 74/179 [00:49<01:07,  1.56it/s]\u001b[A\n","train loss: 6.044:  41%|████▏     | 74/179 [00:50<01:07,  1.56it/s]\u001b[A\n","train loss: 6.044:  42%|████▏     | 75/179 [00:50<01:04,  1.61it/s]\u001b[A\n","train loss: 5.887:  42%|████▏     | 75/179 [00:51<01:04,  1.61it/s]\u001b[A\n","train loss: 5.887:  42%|████▏     | 76/179 [00:51<01:06,  1.56it/s]\u001b[A\n","train loss: 6.030:  42%|████▏     | 76/179 [00:51<01:06,  1.56it/s]\u001b[A\n","train loss: 6.030:  43%|████▎     | 77/179 [00:51<01:02,  1.63it/s]\u001b[A\n","train loss: 6.505:  43%|████▎     | 77/179 [00:52<01:02,  1.63it/s]\u001b[A\n","train loss: 6.505:  44%|████▎     | 78/179 [00:52<01:03,  1.60it/s]\u001b[A\n","train loss: 5.768:  44%|████▎     | 78/179 [00:52<01:03,  1.60it/s]\u001b[A\n","train loss: 5.768:  44%|████▍     | 79/179 [00:52<01:00,  1.65it/s]\u001b[A\n","train loss: 5.928:  44%|████▍     | 79/179 [00:53<01:00,  1.65it/s]\u001b[A\n","train loss: 5.928:  45%|████▍     | 80/179 [00:53<00:59,  1.68it/s]\u001b[A\n","train loss: 5.981:  45%|████▍     | 80/179 [00:54<00:59,  1.68it/s]\u001b[A\n","train loss: 5.981:  45%|████▌     | 81/179 [00:54<01:00,  1.61it/s]\u001b[A\n","train loss: 5.960:  45%|████▌     | 81/179 [00:54<01:00,  1.61it/s]\u001b[A\n","train loss: 5.960:  46%|████▌     | 82/179 [00:54<00:58,  1.66it/s]\u001b[A\n","train loss: 5.999:  46%|████▌     | 82/179 [00:55<00:58,  1.66it/s]\u001b[A\n","train loss: 5.999:  46%|████▋     | 83/179 [00:55<00:58,  1.64it/s]\u001b[A\n","train loss: 5.926:  46%|████▋     | 83/179 [00:55<00:58,  1.64it/s]\u001b[A\n","train loss: 5.926:  47%|████▋     | 84/179 [00:55<00:59,  1.60it/s]\u001b[A\n","train loss: 5.766:  47%|████▋     | 84/179 [00:56<00:59,  1.60it/s]\u001b[A\n","train loss: 5.766:  47%|████▋     | 85/179 [00:56<01:04,  1.45it/s]\u001b[A\n","train loss: 6.035:  47%|████▋     | 85/179 [00:57<01:04,  1.45it/s]\u001b[A\n","train loss: 6.035:  48%|████▊     | 86/179 [00:57<01:00,  1.54it/s]\u001b[A\n","train loss: 5.791:  48%|████▊     | 86/179 [00:57<01:00,  1.54it/s]\u001b[A\n","train loss: 5.791:  49%|████▊     | 87/179 [00:57<01:00,  1.53it/s]\u001b[A\n","train loss: 5.836:  49%|████▊     | 87/179 [00:58<01:00,  1.53it/s]\u001b[A\n","train loss: 5.836:  49%|████▉     | 88/179 [00:58<00:57,  1.59it/s]\u001b[A\n","train loss: 5.769:  49%|████▉     | 88/179 [00:59<00:57,  1.59it/s]\u001b[A\n","train loss: 5.769:  50%|████▉     | 89/179 [00:59<01:01,  1.46it/s]\u001b[A\n","train loss: 5.919:  50%|████▉     | 89/179 [01:00<01:01,  1.46it/s]\u001b[A\n","train loss: 5.919:  50%|█████     | 90/179 [01:00<01:00,  1.46it/s]\u001b[A\n","train loss: 5.954:  50%|█████     | 90/179 [01:00<01:00,  1.46it/s]\u001b[A\n","train loss: 5.954:  51%|█████     | 91/179 [01:00<01:04,  1.36it/s]\u001b[A\n","train loss: 6.171:  51%|█████     | 91/179 [01:01<01:04,  1.36it/s]\u001b[A\n","train loss: 6.171:  51%|█████▏    | 92/179 [01:01<01:00,  1.44it/s]\u001b[A\n","train loss: 5.830:  51%|█████▏    | 92/179 [01:02<01:00,  1.44it/s]\u001b[A\n","train loss: 5.830:  52%|█████▏    | 93/179 [01:02<00:57,  1.48it/s]\u001b[A\n","train loss: 5.948:  52%|█████▏    | 93/179 [01:02<00:57,  1.48it/s]\u001b[A\n","train loss: 5.948:  53%|█████▎    | 94/179 [01:02<00:56,  1.50it/s]\u001b[A\n","train loss: 5.912:  53%|█████▎    | 94/179 [01:03<00:56,  1.50it/s]\u001b[A\n","train loss: 5.912:  53%|█████▎    | 95/179 [01:03<00:56,  1.48it/s]\u001b[A\n","train loss: 6.304:  53%|█████▎    | 95/179 [01:04<00:56,  1.48it/s]\u001b[A\n","train loss: 6.304:  54%|█████▎    | 96/179 [01:04<00:58,  1.41it/s]\u001b[A\n","train loss: 5.749:  54%|█████▎    | 96/179 [01:04<00:58,  1.41it/s]\u001b[A\n","train loss: 5.749:  54%|█████▍    | 97/179 [01:04<00:56,  1.46it/s]\u001b[A\n","train loss: 5.905:  54%|█████▍    | 97/179 [01:05<00:56,  1.46it/s]\u001b[A\n","train loss: 5.905:  55%|█████▍    | 98/179 [01:05<00:55,  1.45it/s]\u001b[A\n","train loss: 5.840:  55%|█████▍    | 98/179 [01:06<00:55,  1.45it/s]\u001b[A\n","train loss: 5.840:  55%|█████▌    | 99/179 [01:06<00:55,  1.43it/s]\u001b[A\n","train loss: 5.884:  55%|█████▌    | 99/179 [01:06<00:55,  1.43it/s]\u001b[A\n","train loss: 5.884:  56%|█████▌    | 100/179 [01:06<00:54,  1.46it/s]\u001b[A\n","train loss: 5.859:  56%|█████▌    | 100/179 [01:07<00:54,  1.46it/s]\u001b[A\n","train loss: 5.859:  56%|█████▋    | 101/179 [01:07<00:49,  1.56it/s]\u001b[A\n","train loss: 5.915:  56%|█████▋    | 101/179 [01:08<00:49,  1.56it/s]\u001b[A\n","train loss: 5.915:  57%|█████▋    | 102/179 [01:08<00:51,  1.50it/s]\u001b[A\n","train loss: 5.893:  57%|█████▋    | 102/179 [01:09<00:51,  1.50it/s]\u001b[A\n","train loss: 5.893:  58%|█████▊    | 103/179 [01:09<01:01,  1.25it/s]\u001b[A\n","train loss: 5.840:  58%|█████▊    | 103/179 [01:09<01:01,  1.25it/s]\u001b[A\n","train loss: 5.840:  58%|█████▊    | 104/179 [01:09<00:57,  1.31it/s]\u001b[A\n","train loss: 5.661:  58%|█████▊    | 104/179 [01:10<00:57,  1.31it/s]\u001b[A\n","train loss: 5.661:  59%|█████▊    | 105/179 [01:10<00:54,  1.36it/s]\u001b[A\n","train loss: 5.732:  59%|█████▊    | 105/179 [01:11<00:54,  1.36it/s]\u001b[A\n","train loss: 5.732:  59%|█████▉    | 106/179 [01:11<00:51,  1.41it/s]\u001b[A\n","train loss: 5.777:  59%|█████▉    | 106/179 [01:12<00:51,  1.41it/s]\u001b[A\n","train loss: 5.777:  60%|█████▉    | 107/179 [01:12<00:52,  1.37it/s]\u001b[A\n","train loss: 5.883:  60%|█████▉    | 107/179 [01:12<00:52,  1.37it/s]\u001b[A\n","train loss: 5.883:  60%|██████    | 108/179 [01:12<00:51,  1.38it/s]\u001b[A\n","train loss: 5.609:  60%|██████    | 108/179 [01:13<00:51,  1.38it/s]\u001b[A\n","train loss: 5.609:  61%|██████    | 109/179 [01:13<00:50,  1.39it/s]\u001b[A\n","train loss: 5.883:  61%|██████    | 109/179 [01:14<00:50,  1.39it/s]\u001b[A\n","train loss: 5.883:  61%|██████▏   | 110/179 [01:14<00:49,  1.41it/s]\u001b[A\n","train loss: 5.873:  61%|██████▏   | 110/179 [01:14<00:49,  1.41it/s]\u001b[A\n","train loss: 5.873:  62%|██████▏   | 111/179 [01:14<00:46,  1.45it/s]\u001b[A\n","train loss: 5.736:  62%|██████▏   | 111/179 [01:15<00:46,  1.45it/s]\u001b[A\n","train loss: 5.736:  63%|██████▎   | 112/179 [01:15<00:42,  1.59it/s]\u001b[A\n","train loss: 5.900:  63%|██████▎   | 112/179 [01:16<00:42,  1.59it/s]\u001b[A\n","train loss: 5.900:  63%|██████▎   | 113/179 [01:16<00:46,  1.43it/s]\u001b[A\n","train loss: 5.807:  63%|██████▎   | 113/179 [01:16<00:46,  1.43it/s]\u001b[A\n","train loss: 5.807:  64%|██████▎   | 114/179 [01:16<00:42,  1.53it/s]\u001b[A\n","train loss: 6.033:  64%|██████▎   | 114/179 [01:17<00:42,  1.53it/s]\u001b[A\n","train loss: 6.033:  64%|██████▍   | 115/179 [01:17<00:42,  1.52it/s]\u001b[A\n","train loss: 5.748:  64%|██████▍   | 115/179 [01:18<00:42,  1.52it/s]\u001b[A\n","train loss: 5.748:  65%|██████▍   | 116/179 [01:18<00:51,  1.23it/s]\u001b[A\n","train loss: 5.704:  65%|██████▍   | 116/179 [01:19<00:51,  1.23it/s]\u001b[A\n","train loss: 5.704:  65%|██████▌   | 117/179 [01:19<00:45,  1.37it/s]\u001b[A\n","train loss: 5.708:  65%|██████▌   | 117/179 [01:19<00:45,  1.37it/s]\u001b[A\n","train loss: 5.708:  66%|██████▌   | 118/179 [01:19<00:41,  1.47it/s]\u001b[A\n","train loss: 5.713:  66%|██████▌   | 118/179 [01:20<00:41,  1.47it/s]\u001b[A\n","train loss: 5.713:  66%|██████▋   | 119/179 [01:20<00:40,  1.48it/s]\u001b[A\n","train loss: 5.566:  66%|██████▋   | 119/179 [01:20<00:40,  1.48it/s]\u001b[A\n","train loss: 5.566:  67%|██████▋   | 120/179 [01:20<00:39,  1.51it/s]\u001b[A\n","train loss: 5.737:  67%|██████▋   | 120/179 [01:21<00:39,  1.51it/s]\u001b[A\n","train loss: 5.737:  68%|██████▊   | 121/179 [01:21<00:39,  1.47it/s]\u001b[A\n","train loss: 5.941:  68%|██████▊   | 121/179 [01:22<00:39,  1.47it/s]\u001b[A\n","train loss: 5.941:  68%|██████▊   | 122/179 [01:22<00:38,  1.49it/s]\u001b[A\n","train loss: 6.000:  68%|██████▊   | 122/179 [01:22<00:38,  1.49it/s]\u001b[A\n","train loss: 6.000:  69%|██████▊   | 123/179 [01:22<00:36,  1.53it/s]\u001b[A\n","train loss: 5.797:  69%|██████▊   | 123/179 [01:23<00:36,  1.53it/s]\u001b[A\n","train loss: 5.797:  69%|██████▉   | 124/179 [01:23<00:35,  1.56it/s]\u001b[A\n","train loss: 5.974:  69%|██████▉   | 124/179 [01:24<00:35,  1.56it/s]\u001b[A\n","train loss: 5.974:  70%|██████▉   | 125/179 [01:24<00:36,  1.49it/s]\u001b[A\n","train loss: 5.706:  70%|██████▉   | 125/179 [01:24<00:36,  1.49it/s]\u001b[A\n","train loss: 5.706:  70%|███████   | 126/179 [01:24<00:33,  1.57it/s]\u001b[A\n","train loss: 5.840:  70%|███████   | 126/179 [01:25<00:33,  1.57it/s]\u001b[A\n","train loss: 5.840:  71%|███████   | 127/179 [01:25<00:33,  1.53it/s]\u001b[A\n","train loss: 5.959:  71%|███████   | 127/179 [01:26<00:33,  1.53it/s]\u001b[A\n","train loss: 5.959:  72%|███████▏  | 128/179 [01:26<00:31,  1.63it/s]\u001b[A\n","train loss: 5.640:  72%|███████▏  | 128/179 [01:26<00:31,  1.63it/s]\u001b[A\n","train loss: 5.640:  72%|███████▏  | 129/179 [01:26<00:29,  1.70it/s]\u001b[A\n","train loss: 5.707:  72%|███████▏  | 129/179 [01:27<00:29,  1.70it/s]\u001b[A\n","train loss: 5.707:  73%|███████▎  | 130/179 [01:27<00:30,  1.59it/s]\u001b[A\n","train loss: 5.893:  73%|███████▎  | 130/179 [01:28<00:30,  1.59it/s]\u001b[A\n","train loss: 5.893:  73%|███████▎  | 131/179 [01:28<00:31,  1.55it/s]\u001b[A\n","train loss: 5.654:  73%|███████▎  | 131/179 [01:29<00:31,  1.55it/s]\u001b[A\n","train loss: 5.654:  74%|███████▎  | 132/179 [01:29<00:37,  1.26it/s]\u001b[A\n","train loss: 5.900:  74%|███████▎  | 132/179 [01:29<00:37,  1.26it/s]\u001b[A\n","train loss: 5.900:  74%|███████▍  | 133/179 [01:29<00:34,  1.34it/s]\u001b[A\n","train loss: 5.834:  74%|███████▍  | 133/179 [01:30<00:34,  1.34it/s]\u001b[A\n","train loss: 5.834:  75%|███████▍  | 134/179 [01:30<00:32,  1.39it/s]\u001b[A\n","train loss: 5.658:  75%|███████▍  | 134/179 [01:30<00:32,  1.39it/s]\u001b[A\n","train loss: 5.658:  75%|███████▌  | 135/179 [01:30<00:29,  1.50it/s]\u001b[A\n","train loss: 5.758:  75%|███████▌  | 135/179 [01:31<00:29,  1.50it/s]\u001b[A\n","train loss: 5.758:  76%|███████▌  | 136/179 [01:31<00:29,  1.46it/s]\u001b[A\n","train loss: 5.781:  76%|███████▌  | 136/179 [01:32<00:29,  1.46it/s]\u001b[A\n","train loss: 5.781:  77%|███████▋  | 137/179 [01:32<00:26,  1.57it/s]\u001b[A\n","train loss: 5.766:  77%|███████▋  | 137/179 [01:32<00:26,  1.57it/s]\u001b[A\n","train loss: 5.766:  77%|███████▋  | 138/179 [01:32<00:26,  1.55it/s]\u001b[A\n","train loss: 5.783:  77%|███████▋  | 138/179 [01:33<00:26,  1.55it/s]\u001b[A\n","train loss: 5.783:  78%|███████▊  | 139/179 [01:33<00:25,  1.56it/s]\u001b[A\n","train loss: 5.723:  78%|███████▊  | 139/179 [01:34<00:25,  1.56it/s]\u001b[A\n","train loss: 5.723:  78%|███████▊  | 140/179 [01:34<00:23,  1.63it/s]\u001b[A\n","train loss: 5.725:  78%|███████▊  | 140/179 [01:34<00:23,  1.63it/s]\u001b[A\n","train loss: 5.725:  79%|███████▉  | 141/179 [01:34<00:26,  1.42it/s]\u001b[A\n","train loss: 5.832:  79%|███████▉  | 141/179 [01:35<00:26,  1.42it/s]\u001b[A\n","train loss: 5.832:  79%|███████▉  | 142/179 [01:35<00:25,  1.45it/s]\u001b[A\n","train loss: 5.961:  79%|███████▉  | 142/179 [01:36<00:25,  1.45it/s]\u001b[A\n","train loss: 5.961:  80%|███████▉  | 143/179 [01:36<00:23,  1.56it/s]\u001b[A\n","train loss: 5.644:  80%|███████▉  | 143/179 [01:36<00:23,  1.56it/s]\u001b[A\n","train loss: 5.644:  80%|████████  | 144/179 [01:36<00:22,  1.56it/s]\u001b[A\n","train loss: 5.816:  80%|████████  | 144/179 [01:37<00:22,  1.56it/s]\u001b[A\n","train loss: 5.816:  81%|████████  | 145/179 [01:37<00:23,  1.44it/s]\u001b[A\n","train loss: 5.784:  81%|████████  | 145/179 [01:38<00:23,  1.44it/s]\u001b[A\n","train loss: 5.784:  82%|████████▏ | 146/179 [01:38<00:23,  1.41it/s]\u001b[A\n","train loss: 5.632:  82%|████████▏ | 146/179 [01:39<00:23,  1.41it/s]\u001b[A\n","train loss: 5.632:  82%|████████▏ | 147/179 [01:39<00:22,  1.39it/s]\u001b[A\n","train loss: 5.743:  82%|████████▏ | 147/179 [01:39<00:22,  1.39it/s]\u001b[A\n","train loss: 5.743:  83%|████████▎ | 148/179 [01:39<00:22,  1.36it/s]\u001b[A\n","train loss: 5.738:  83%|████████▎ | 148/179 [01:40<00:22,  1.36it/s]\u001b[A\n","train loss: 5.738:  83%|████████▎ | 149/179 [01:40<00:20,  1.46it/s]\u001b[A\n","train loss: 5.809:  83%|████████▎ | 149/179 [01:41<00:20,  1.46it/s]\u001b[A\n","train loss: 5.809:  84%|████████▍ | 150/179 [01:41<00:19,  1.48it/s]\u001b[A\n","train loss: 5.806:  84%|████████▍ | 150/179 [01:41<00:19,  1.48it/s]\u001b[A\n","train loss: 5.806:  84%|████████▍ | 151/179 [01:41<00:17,  1.57it/s]\u001b[A\n","train loss: 5.489:  84%|████████▍ | 151/179 [01:42<00:17,  1.57it/s]\u001b[A\n","train loss: 5.489:  85%|████████▍ | 152/179 [01:42<00:17,  1.55it/s]\u001b[A\n","train loss: 5.904:  85%|████████▍ | 152/179 [01:42<00:17,  1.55it/s]\u001b[A\n","train loss: 5.904:  85%|████████▌ | 153/179 [01:42<00:16,  1.60it/s]\u001b[A\n","train loss: 5.838:  85%|████████▌ | 153/179 [01:43<00:16,  1.60it/s]\u001b[A\n","train loss: 5.838:  86%|████████▌ | 154/179 [01:43<00:15,  1.58it/s]\u001b[A\n","train loss: 5.707:  86%|████████▌ | 154/179 [01:44<00:15,  1.58it/s]\u001b[A\n","train loss: 5.707:  87%|████████▋ | 155/179 [01:44<00:15,  1.56it/s]\u001b[A\n","train loss: 5.644:  87%|████████▋ | 155/179 [01:45<00:15,  1.56it/s]\u001b[A\n","train loss: 5.644:  87%|████████▋ | 156/179 [01:45<00:15,  1.47it/s]\u001b[A\n","train loss: 5.706:  87%|████████▋ | 156/179 [01:45<00:15,  1.47it/s]\u001b[A\n","train loss: 5.706:  88%|████████▊ | 157/179 [01:45<00:14,  1.53it/s]\u001b[A\n","train loss: 5.772:  88%|████████▊ | 157/179 [01:46<00:14,  1.53it/s]\u001b[A\n","train loss: 5.772:  88%|████████▊ | 158/179 [01:46<00:13,  1.59it/s]\u001b[A\n","train loss: 5.962:  88%|████████▊ | 158/179 [01:46<00:13,  1.59it/s]\u001b[A\n","train loss: 5.962:  89%|████████▉ | 159/179 [01:46<00:11,  1.67it/s]\u001b[A\n","train loss: 5.590:  89%|████████▉ | 159/179 [01:47<00:11,  1.67it/s]\u001b[A\n","train loss: 5.590:  89%|████████▉ | 160/179 [01:47<00:11,  1.68it/s]\u001b[A\n","train loss: 5.846:  89%|████████▉ | 160/179 [01:47<00:11,  1.68it/s]\u001b[A\n","train loss: 5.846:  90%|████████▉ | 161/179 [01:47<00:10,  1.64it/s]\u001b[A\n","train loss: 5.795:  90%|████████▉ | 161/179 [01:48<00:10,  1.64it/s]\u001b[A\n","train loss: 5.795:  91%|█████████ | 162/179 [01:48<00:10,  1.64it/s]\u001b[A\n","train loss: 5.765:  91%|█████████ | 162/179 [01:49<00:10,  1.64it/s]\u001b[A\n","train loss: 5.765:  91%|█████████ | 163/179 [01:49<00:09,  1.73it/s]\u001b[A\n","train loss: 5.737:  91%|█████████ | 163/179 [01:49<00:09,  1.73it/s]\u001b[A\n","train loss: 5.737:  92%|█████████▏| 164/179 [01:49<00:08,  1.69it/s]\u001b[A\n","train loss: 6.000:  92%|█████████▏| 164/179 [01:50<00:08,  1.69it/s]\u001b[A\n","train loss: 6.000:  92%|█████████▏| 165/179 [01:50<00:08,  1.64it/s]\u001b[A\n","train loss: 5.757:  92%|█████████▏| 165/179 [01:50<00:08,  1.64it/s]\u001b[A\n","train loss: 5.757:  93%|█████████▎| 166/179 [01:50<00:08,  1.62it/s]\u001b[A\n","train loss: 5.781:  93%|█████████▎| 166/179 [01:51<00:08,  1.62it/s]\u001b[A\n","train loss: 5.781:  93%|█████████▎| 167/179 [01:51<00:07,  1.65it/s]\u001b[A\n","train loss: 5.845:  93%|█████████▎| 167/179 [01:52<00:07,  1.65it/s]\u001b[A\n","train loss: 5.845:  94%|█████████▍| 168/179 [01:52<00:06,  1.68it/s]\u001b[A\n","train loss: 5.743:  94%|█████████▍| 168/179 [01:52<00:06,  1.68it/s]\u001b[A\n","train loss: 5.743:  94%|█████████▍| 169/179 [01:52<00:06,  1.58it/s]\u001b[A\n","train loss: 5.654:  94%|█████████▍| 169/179 [01:53<00:06,  1.58it/s]\u001b[A\n","train loss: 5.654:  95%|█████████▍| 170/179 [01:53<00:05,  1.60it/s]\u001b[A\n","train loss: 5.593:  95%|█████████▍| 170/179 [01:54<00:05,  1.60it/s]\u001b[A\n","train loss: 5.593:  96%|█████████▌| 171/179 [01:54<00:05,  1.58it/s]\u001b[A\n","train loss: 5.692:  96%|█████████▌| 171/179 [01:54<00:05,  1.58it/s]\u001b[A\n","train loss: 5.692:  96%|█████████▌| 172/179 [01:54<00:04,  1.50it/s]\u001b[A\n","train loss: 5.797:  96%|█████████▌| 172/179 [01:55<00:04,  1.50it/s]\u001b[A\n","train loss: 5.797:  97%|█████████▋| 173/179 [01:55<00:03,  1.53it/s]\u001b[A\n","train loss: 5.866:  97%|█████████▋| 173/179 [01:56<00:03,  1.53it/s]\u001b[A\n","train loss: 5.866:  97%|█████████▋| 174/179 [01:56<00:03,  1.47it/s]\u001b[A\n","train loss: 5.715:  97%|█████████▋| 174/179 [01:56<00:03,  1.47it/s]\u001b[A\n","train loss: 5.715:  98%|█████████▊| 175/179 [01:56<00:02,  1.44it/s]\u001b[A\n","train loss: 5.805:  98%|█████████▊| 175/179 [01:57<00:02,  1.44it/s]\u001b[A\n","train loss: 5.805:  98%|█████████▊| 176/179 [01:57<00:02,  1.44it/s]\u001b[A\n","train loss: 5.666:  98%|█████████▊| 176/179 [01:58<00:02,  1.44it/s]\u001b[A\n","train loss: 5.666:  99%|█████████▉| 177/179 [01:58<00:01,  1.54it/s]\u001b[A\n","train loss: 5.647:  99%|█████████▉| 177/179 [01:58<00:01,  1.54it/s]\u001b[A\n","train loss: 5.647:  99%|█████████▉| 178/179 [01:58<00:00,  1.61it/s]\u001b[A\n","train loss: 5.854:  99%|█████████▉| 178/179 [01:59<00:00,  1.61it/s]\u001b[A\n","train loss: 5.854: 100%|██████████| 179/179 [01:59<00:00,  1.50it/s]\u001b[A\n","validating:   0%|          | 0/1 [02:00<?, ?it/s]\n","  0%|          | 0/30 [00:00<?, ?it/s]\u001b[AYou're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/tmp/ipykernel_31/2932555708.py:96: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","\n","valid loss: 5.515:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n","valid loss: 5.515:   3%|▎         | 1/30 [00:00<00:16,  1.77it/s]\u001b[A\n","valid loss: 5.518:   3%|▎         | 1/30 [00:00<00:16,  1.77it/s]\u001b[A\n","valid loss: 5.518:   7%|▋         | 2/30 [00:00<00:11,  2.35it/s]\u001b[A\n","valid loss: 5.399:   7%|▋         | 2/30 [00:01<00:11,  2.35it/s]\u001b[A\n","valid loss: 5.399:  10%|█         | 3/30 [00:01<00:11,  2.37it/s]\u001b[A\n","valid loss: 5.530:  10%|█         | 3/30 [00:01<00:11,  2.37it/s]\u001b[A\n","valid loss: 5.530:  13%|█▎        | 4/30 [00:01<00:10,  2.47it/s]\u001b[A\n","valid loss: 5.448:  13%|█▎        | 4/30 [00:01<00:10,  2.47it/s]\u001b[A\n","valid loss: 5.448:  17%|█▋        | 5/30 [00:01<00:08,  2.83it/s]\u001b[A\n","valid loss: 5.607:  17%|█▋        | 5/30 [00:02<00:08,  2.83it/s]\u001b[A\n","valid loss: 5.607:  20%|██        | 6/30 [00:02<00:08,  2.95it/s]\u001b[A\n","valid loss: 5.484:  20%|██        | 6/30 [00:02<00:08,  2.95it/s]\u001b[A\n","valid loss: 5.484:  23%|██▎       | 7/30 [00:02<00:07,  2.89it/s]\u001b[A\n","valid loss: 5.576:  23%|██▎       | 7/30 [00:02<00:07,  2.89it/s]\u001b[A\n","valid loss: 5.576:  27%|██▋       | 8/30 [00:02<00:07,  3.02it/s]\u001b[A\n","valid loss: 5.459:  27%|██▋       | 8/30 [00:03<00:07,  3.02it/s]\u001b[A\n","valid loss: 5.459:  30%|███       | 9/30 [00:03<00:07,  2.89it/s]\u001b[A\n","valid loss: 5.415:  30%|███       | 9/30 [00:03<00:07,  2.89it/s]\u001b[A\n","valid loss: 5.415:  33%|███▎      | 10/30 [00:03<00:07,  2.80it/s]\u001b[A\n","valid loss: 5.558:  33%|███▎      | 10/30 [00:04<00:07,  2.80it/s]\u001b[A\n","valid loss: 5.558:  37%|███▋      | 11/30 [00:04<00:06,  2.84it/s]\u001b[A\n","valid loss: 5.621:  37%|███▋      | 11/30 [00:04<00:06,  2.84it/s]\u001b[A\n","valid loss: 5.621:  40%|████      | 12/30 [00:04<00:06,  2.97it/s]\u001b[A\n","valid loss: 5.973:  40%|████      | 12/30 [00:04<00:06,  2.97it/s]\u001b[A\n","valid loss: 5.973:  43%|████▎     | 13/30 [00:04<00:05,  2.96it/s]\u001b[A\n","valid loss: 6.303:  43%|████▎     | 13/30 [00:05<00:05,  2.96it/s]\u001b[A\n","valid loss: 6.303:  47%|████▋     | 14/30 [00:05<00:05,  2.83it/s]\u001b[A\n","valid loss: 5.680:  47%|████▋     | 14/30 [00:05<00:05,  2.83it/s]\u001b[A\n","valid loss: 5.680:  50%|█████     | 15/30 [00:05<00:05,  2.86it/s]\u001b[A\n","valid loss: 5.748:  50%|█████     | 15/30 [00:05<00:05,  2.86it/s]\u001b[A\n","valid loss: 5.748:  53%|█████▎    | 16/30 [00:05<00:04,  2.87it/s]\u001b[A\n","valid loss: 5.689:  53%|█████▎    | 16/30 [00:06<00:04,  2.87it/s]\u001b[A\n","valid loss: 5.689:  57%|█████▋    | 17/30 [00:06<00:05,  2.54it/s]\u001b[A\n","valid loss: 5.685:  57%|█████▋    | 17/30 [00:06<00:05,  2.54it/s]\u001b[A\n","valid loss: 5.685:  60%|██████    | 18/30 [00:06<00:04,  2.53it/s]\u001b[A\n","valid loss: 5.630:  60%|██████    | 18/30 [00:06<00:04,  2.53it/s]\u001b[A\n","valid loss: 5.630:  63%|██████▎   | 19/30 [00:07<00:04,  2.60it/s]\u001b[A\n","valid loss: 5.625:  63%|██████▎   | 19/30 [00:07<00:04,  2.60it/s]\u001b[A\n","valid loss: 5.625:  67%|██████▋   | 20/30 [00:07<00:03,  2.65it/s]\u001b[A\n","valid loss: 5.471:  67%|██████▋   | 20/30 [00:07<00:03,  2.65it/s]\u001b[A\n","valid loss: 5.471:  70%|███████   | 21/30 [00:07<00:03,  2.91it/s]\u001b[A\n","valid loss: 5.554:  70%|███████   | 21/30 [00:07<00:03,  2.91it/s]\u001b[A\n","valid loss: 5.554:  73%|███████▎  | 22/30 [00:07<00:02,  2.89it/s]\u001b[A\n","valid loss: 5.516:  73%|███████▎  | 22/30 [00:08<00:02,  2.89it/s]\u001b[A\n","valid loss: 5.516:  77%|███████▋  | 23/30 [00:08<00:02,  2.88it/s]\u001b[A\n","valid loss: 5.603:  77%|███████▋  | 23/30 [00:08<00:02,  2.88it/s]\u001b[A\n","valid loss: 5.603:  80%|████████  | 24/30 [00:08<00:01,  3.05it/s]\u001b[A\n","valid loss: 5.757:  80%|████████  | 24/30 [00:08<00:01,  3.05it/s]\u001b[A\n","valid loss: 5.757:  83%|████████▎ | 25/30 [00:08<00:01,  3.21it/s]\u001b[A\n","valid loss: 5.704:  83%|████████▎ | 25/30 [00:09<00:01,  3.21it/s]\u001b[A\n","valid loss: 5.704:  87%|████████▋ | 26/30 [00:09<00:01,  3.32it/s]\u001b[A\n","valid loss: 5.356:  87%|████████▋ | 26/30 [00:09<00:01,  3.32it/s]\u001b[A\n","valid loss: 5.356:  90%|█████████ | 27/30 [00:09<00:00,  3.24it/s]\u001b[A\n","valid loss: 5.725:  90%|█████████ | 27/30 [00:09<00:00,  3.24it/s]\u001b[A\n","valid loss: 5.725:  93%|█████████▎| 28/30 [00:09<00:00,  3.32it/s]\u001b[A\n","valid loss: 5.496:  93%|█████████▎| 28/30 [00:10<00:00,  3.32it/s]\u001b[A\n","valid loss: 5.496:  97%|█████████▋| 29/30 [00:10<00:00,  2.87it/s]\u001b[A\n","valid loss: 5.475:  97%|█████████▋| 29/30 [00:10<00:00,  2.87it/s]\u001b[A\n","valid loss: 5.475: 100%|██████████| 30/30 [00:10<00:00,  2.87it/s]\u001b[A\n"]},{"name":"stdout","output_type":"stream","text":["  train_loss train_perplexity val_loss val_perplexity\n","0   6.500959       665.779517  5.60402     271.515759\n","saving best model...\n"]},{"name":"stderr","output_type":"stream","text":["validating: 100%|██████████| 1/1 [02:12<00:00, 132.29s/it]\n"]},{"data":{"text/plain":["{'best_perplexity': 271.515759109497, 'best_epoch': 0}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"execution_count":13},{"id":"4ac6d623","cell_type":"code","source":"trainer.metrics","metadata":{"execution":{"iopub.execute_input":"2025-04-14T05:29:20.359536Z","iopub.status.busy":"2025-04-14T05:29:20.359238Z","iopub.status.idle":"2025-04-14T05:29:20.375459Z","shell.execute_reply":"2025-04-14T05:29:20.374737Z","shell.execute_reply.started":"2025-04-14T05:29:20.359507Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>train_loss</th>\n","      <th>train_perplexity</th>\n","      <th>val_loss</th>\n","      <th>val_perplexity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6.500959</td>\n","      <td>665.779517</td>\n","      <td>5.60402</td>\n","      <td>271.515759</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  train_loss train_perplexity val_loss val_perplexity\n","0   6.500959       665.779517  5.60402     271.515759"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"execution_count":14},{"id":"13c64648-dc2a-42bc-965f-cac8d70b3ddc","cell_type":"markdown","source":"### Evaluation on Test Set\n\nThis section outlines the process for evaluating the model on the test set. The evaluation involves generating captions for images in the test dataset and computing several common evaluation metrics: BLEU, METEOR, and ROUGE-L.\n\n1. **Prepare the Test Dataset**:\n   - The test dataset is loaded using a custom `CustomCaptionDataset` class, which reads the CSV file (`test.csv`) and the images from the specified directory.\n   - The dataset is tokenized using the pre-trained tokenizer, and any necessary transformations (such as resizing or normalization) are applied to the images.\n\n2. **Load the Best Model**:\n   - The best model (based on validation performance) is loaded using the `trainer.load_best_model()` method.\n\n3. **Setup Evaluation Metrics**:\n   - The following evaluation metrics are loaded using the `evaluate` library:\n     - **BLEU**: Measures the precision of n-grams in the generated captions compared to the reference captions.\n     - **METEOR**: Computes the harmonic mean of precision and recall, considering synonymy and stemming.\n     - **ROUGE-L**: Measures the longest common subsequence between the generated and reference captions.\n\n4. **Generate Captions**:\n   - For each image in the test dataset, the model generates a caption. The generated caption is stored in the `predictions` list, while the reference caption is stored in the `references` list. Note that BLEU and METEOR expect the references to be in a list format.\n\n5. **Evaluate the Model**:\n   - The BLEU, METEOR, and ROUGE-L scores are computed by comparing the model's predictions with the ground truth references.\n\n6. **Report Results**:\n   - Finally, the evaluation scores are printed, providing insight into the model's performance on the test set. The BLEU, METEOR, and ROUGE-L scores indicate the quality of the generated captions.\n","metadata":{}},{"id":"8bbfa640","cell_type":"code","source":"import evaluate\nfrom tqdm import tqdm\n\n# Step 1: Prepare the test dataset\ntest_ds = CustomCaptionDataset(\n    csv_path=os.path.join(root_dir, \"test.csv\"),\n    image_dir=os.path.join(root_dir, \"test\"),\n    tokenizer=tokenizer,\n    transform=valid_tfms\n)\n\n# Step 2: Load the best model\ntrainer.load_best_model()\n\n# Step 3: Setup evaluation metrics\nbleu = evaluate.load(\"bleu\")\nmeteor = evaluate.load(\"meteor\")\nrouge = evaluate.load(\"rouge\")\n\npredictions = []\nreferences = []\n\n# Step 4: Generate captions\nfor i in tqdm(range(len(test_ds))):\n    image_path = test_ds.df.iloc[i]['filename']\n    full_path = os.path.join(test_ds.image_dir, image_path)\n    \n    ref_caption = test_ds.df.iloc[i]['caption']\n    gen_caption = trainer.generate_caption(full_path)\n\n    predictions.append(gen_caption.strip())\n    references.append([ref_caption.strip()])  # BLEU and METEOR need list of references\n\n# Step 5: Evaluate\nbleu_score = bleu.compute(predictions=predictions, references=references)\nmeteor_score = meteor.compute(predictions=predictions, references=references)\nrouge_score = rouge.compute(predictions=predictions, references=[r[0] for r in references])\n\n# Step 6: Report\nprint(\"\\n=== Evaluation on Test Set ===\")\nprint(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\nprint(f\"METEOR Score: {meteor_score['meteor']:.4f}\")\nprint(f\"ROUGE-L Score: {rouge_score['rougeL']:.4f}\")\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T05:29:20.699449Z","iopub.status.busy":"2025-04-14T05:29:20.699227Z","iopub.status.idle":"2025-04-14T05:40:03.020206Z","shell.execute_reply":"2025-04-14T05:40:03.019248Z","shell.execute_reply.started":"2025-04-14T05:29:20.699422Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_31/2932555708.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  sd = torch.load(self.train_config.model_path/'captioner.pt')\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"106898df568344459ddf8d5dff704831","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"afdbb10be83e401da0afd67023b5ecaf","version_major":2,"version_minor":0},"text/plain":["Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a45bed985bb44f3f9f61e7eb1e905239","version_major":2,"version_minor":0},"text/plain":["Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"462f5228a2114ffd8f7b4139af930cd2","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f99179af13ff4c348d6dee7ac849a8a5","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 928/928 [10:25<00:00,  1.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","=== Evaluation on Test Set ===\n","BLEU Score: 0.0000\n","METEOR Score: 0.1194\n","ROUGE-L Score: 0.1495\n"]}],"execution_count":17},{"id":"b4f7c32d-e513-42c2-97b8-1a6e878fc9e9","cell_type":"markdown","source":"### Load Pretrained SmolVLM Model and Processor\n\nIn this step, we load the pretrained SmolVLM model and its associated processor from the Hugging Face model hub. The processor is responsible for pre-processing the input data (e.g., images and text) to ensure compatibility with the model. The model is loaded using `AutoModelForVision2Seq`, which is a Vision-to-Sequence model designed for tasks like image captioning.\n\n1. **Load the Processor**:\n   - The processor is loaded using `AutoProcessor.from_pretrained()`, which automatically retrieves the processor for the specified pretrained model, `\"HuggingFaceTB/SmolVLM-256M-Instruct\"`. This processor will handle the transformation of input data into a format suitable for the model.\n\n2. **Load the Model**:\n   - The model is loaded using `AutoModelForVision2Seq.from_pretrained()` with the same model identifier (`\"HuggingFaceTB/SmolVLM-256M-Instruct\"`). The model is loaded with the appropriate precision (either `bfloat16` or `float32`) depending on the availability of GPU support.\n   - The model is then moved to the specified device (`cuda` if available, otherwise `cpu`) to ensure optimal computation.\n   - The `_attn_implementation=\"eager\"` argument is set as a fallback for `flash_attention_2` in case it is unavailable.\n\nThe loaded model and processor will be used for inference and image captioning tasks.\n","metadata":{}},{"id":"d8189996","cell_type":"code","source":"\nprocessor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-256M-Instruct\")\nmodel = AutoModelForVision2Seq.from_pretrained(\n    \"HuggingFaceTB/SmolVLM-256M-Instruct\",\n    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n    _attn_implementation=\"eager\",  # fallback for flash_attention_2\n).to(device)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401,"referenced_widgets":["0b22f384f9324313a8e1e08455454d08","849661fa6cc14c0c88ed4358ae9e8dc8","e93c24b50c3242aca0434e5e76dc253c","c93d8e9fcc354206ae0127b26ea46576","72b1b0f425ab43b38914b6b30a117e7e","bfd0feccd2a94437ac62439c6e1aedb1","e254411d3c61411c807711aeb1b40876","e9d406a211f04e05a3b408c30be3ee07","5426149a59e945d4b12e521f13fc15b5","1b74b6dc3ced4856919ce8fea43fd564","02294593f242451da0e0e21dad033b1e","6de5baf3a9784e548aea8adf16901738","0f5395f03fa044198037135db9c0c166","f90f271f4f7f4fcfb1a704539c745fd4","6c7f7a90f19f4f0a843182e47ed7ef5e","068580eb0c7f44118636704d2c848e3c","a9e49056b8ca4190b8096e0a351b79fc","423bd0b4853643da838a819f8027b236","c032d6bb2d1e47b481cfe83b3228f6f1","00c2f8dd3ae148459546733a0b005cd9","ac3140f684c04f30a075ab37309d2592","9bc8dd9b2cf94c5591c12702a305354b","5d6843a9cb164136b109f3a3d06cda64","e1e23ed3a2bc4ab1beca51a867911384","a2fbf145482a4396961a3f17d07e2759","0f035134514144979e9536daa6a91211","51bba3f501574b73bf9e3d0cfa9ceac8","3aa58cc903cc4e3f82f5414cad2d2018","6174a0a2b2784e2a95d95e4ce2044566","e224121428214408848cb141615ac83e","4daaafe19c6f452dbc0b890e4fd38dbf","9b8187f3ee4f4c81a9abf098c297b744","ed00853fcd304c09b25a32f06462d93d","77f5cd4716244cbc94152c9ed12cf6ca","4d18b92088984d6c8a0c87317d287d91","a4874345a7d9467e9614f4f25e1fe15a","e497c92a61ad498aa94a3e9318a0310e","bf3e7fa2ee6a4d19ba080dea78fca9e3","85afde41ab974d548d955c4fbf47fb32","fe71d9f1091f498e9e8a62f919ed2ba5","908051ae2bb74342a6ff7c076368f7e7","94207f8693354df2b4bd12104c9c3ef2","4b3dc89287ae42e98006a746cbe1d422","63be0a45735e411ba74c642a0c883507","7ec5579222784720a2a4ff97a1254811","5d0c605d236e434b9e6314a1b084c364","496fb22fbf9c4702a3f13c9a9d368e6c","d5e63c0b5c194e58b19d50a148382bb5","4b397e86e4af498198ab66e256c8480a","ae0e496f0b144424a2b0664ccbcf9272","121874cd72894a648c673022350a37d7","8afbdf1a89a1467b8b14eb3bb0844f9b","7773ef9833ab4fe9b12c7e514f82b987","c58bc882e6ee4bb3a23e4f6c2331d207","e79cc295f56a4d0697941aceebf6869b","b290d9b4cc7e4599abf55e13ee125af3","3b512a4ed8284167ac1a7b3d0ab1b9bf","0910032ec54e42e490297a2db4d4c5fc","87b89826a4774378aba79fc313bd713b","5d699c57ad2f4789ba86b141d27230a6","21f50809c09f4027b3d1a706ac4c5952","8d9ef42c29644ad9809ff9f33204d2d4","cb08abee8b7a4d9f9c1b44a6a00f26c5","e40698ad93f549abbf97a59e2cc309fe","e7279eff6ff04b24ac06c2315c82f8e2","8df49525fa90431190a87e8fe8da8120","e291c33d99994c079e799e6a97ec04dc","c1cc4cadea28408cb4350e4609059436","3df3c8e1dc7f473581bf537c56217672","6a9c7ca10ffc425d98b6016fa8255895","f5e83b43a0e44facb19b470d9e54d395","0339025854af46e9b61b7b236d1d6ef4","6a50f6d58f1a4fa183829c2819a15f60","bc6c34b1968c4d6a9656a28ff81aabcf","825c80b995b24376885809ef6f4ddee8","4b6307d3fc8241688834356b24a0a4f6","c8bdbae54d044ff594f10ffbce91fb9c","b0dba0b6c2004a98901b84cc0f24195f","dcddafd19d5a45018274f7a6cfdfad5e","5576579e68bf4f1baa451fe684edfd78","39f347d674ef4de5ad3aef62ebd242ba","76e492d9960945a1a08a49e4eafedde0","b94d653ce78e4ab499d541d7bcd3480a","bc7995d6a6424ff1a717add016bb4110","9eb0361212df4345aa417c2d673ea887","5b23610e895b474588cb9efbf2680e19","30c146fcc2124941adb11febe4dcd0e8","0b84917745584f9989c87585d68d0c70","40f8715ccda54b60b133af9399bebd6b","21118bc9ccff4292855a29517951879f","007a159cd5eb46669a4ad61de61df012","db4c37223a524619979f3934721d5c63","92f6b301840c44ec84db5ae58d05d129","2798e35498d048f8ab570c26213ded8b","08288a046190433381fbee25f0372942","ea6d9146f8d04c0c82bfee0c6db70a3f","6551621f9fe54d79bcdb0bb6349ecda4","f307bb50fc0848c6a25701f4d2822cde","7d39daa055ff43a59311b1b1f0f8df63","ce9e394902704eb7b242527c3ab93380","5bb5e08445c74f3499d4abc8ee645077","ced5f157decd430d81c47164c3b3928a","ba182928e3d94add9a10fcde99ad9e14","cb7a94e7e3d941968fb5cc0395218353","4a98dd2be27c4b95afafa6075c5395e8","2d86178ca3b74866b3eeec53c609ac49","083db87879f94e75bd61d3a8c0f875eb","33b7d116f2334945b33fcef0588181ba","923dd4d6f62c43f8afcdfcc26ff0b8d2","1de3b92a76344f368a0b41805f90ebe9","fc54d0514178404cab621bbc9af548b9","7b20bf761b9c468fbc3e6d50e4d71802","339358cc73624cb2839496061969401e","05920796ef9046f480d2fac3c460f802","680ec5dceb32495dad1ce82bf405cd13","941be16ec5654c5ab94b38e0b1f7d3d4","eb7c5d68a3254b8685e375ef8d0b19b5","06e99c372d3c48ddb288313f8778d5d0","000e9007b9b8415890f7591bf0ccb0b4","19e82418dfb2411a914d41fd49d733a4","163bd1a1b36c4918b825ef38a385ac8f","7f737149475b49819186c68874c056c2","ae03672bfb9b4b4b94bb8adf5531770b","be8a0dcc9210416fbc8477e177d89ed9","72892a9a9d9846edb47d2c8eb0ef87e3","b3f3b08930464a2e80af7840a213d410","c1484729d6cf45e389c907f492ffc7c1","0403eb26a6434e45b5003c08b3dd4d8c","f35351e4ebc44a3fbc003dfe8d756fb0","ea0340a9dd984b8ab21d6759207e4a1b","0e8dc66857c64736a7434738a4a7575d","e505ce125c9c41abbd335b7e9ebb56f4"]},"execution":{"iopub.execute_input":"2025-04-14T09:46:23.222530Z","iopub.status.busy":"2025-04-14T09:46:23.221849Z","iopub.status.idle":"2025-04-14T09:46:26.779289Z","shell.execute_reply":"2025-04-14T09:46:26.778746Z","shell.execute_reply.started":"2025-04-14T09:46:23.222505Z"},"id":"d8189996","outputId":"eb3452c3-0d95-436d-8507-0e6f61368a0a","trusted":true},"outputs":[],"execution_count":40},{"id":"8eb09ebb-3880-4287-b147-f28e29afd966","cell_type":"markdown","source":"### Setup Evaluation Metrics and Zero-Shot Captioning\n\nIn this section, we set up the evaluation metrics (BLEU, METEOR, and ROUGE) and define a function for zero-shot image captioning.\n\n1. **Setup BLEU Smoothing**:\n   - The `SmoothingFunction` from the `nltk.translate.bleu_score` module is used to apply a smoothing function to the BLEU score calculation. This helps avoid zero BLEU scores for very short sentences or captions. We use `method4` as the smoothing method.\n\n2. **Setup Evaluation Metrics**:\n   - The evaluation metrics (BLEU, METEOR, and ROUGE) are loaded using the `evaluate` library. These metrics are used to assess the quality of generated captions in comparison to reference captions.\n\n3. **Zero-Shot Captioning**:\n   - The `zero_shot_captioning` function takes an image as input and generates a caption without the need for fine-tuning on a specific dataset. The process involves:\n     - Constructing a prompt asking the model, \"What’s in this image?\"\n     - Applying the processor's `apply_chat_template` method to format the prompt and image for the model.\n     - Generating a caption using the model with a maximum token length of 50.\n     - Cleaning the caption to trim it to just the response (removing the question and assistant label).\n\nThe generated caption provides a description of the image, which can be evaluated using the aforementioned metrics.\n","metadata":{}},{"id":"53ae5048-dd2d-422b-89d2-1277fe7105c2","cell_type":"code","source":"\n\nfrom nltk.translate.bleu_score import SmoothingFunction\nsmooth_fn = SmoothingFunction().method4\n\n\n# Step 2: Setup evaluation metrics\nbleu = evaluate.load(\"bleu\")\nmeteor = evaluate.load(\"meteor\")\nrouge = evaluate.load(\"rouge\")\n\n\ndef zero_shot_captioning(image: Image.Image) -> str:\n    messages = [{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": \"What’s in this image?\"}\n        ]\n    }]\n    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n\n    inputs = processor(text=prompt, images=[image], return_tensors=\"pt\").to(device)\n    generated_ids = model.generate(**inputs, max_new_tokens=50)\n    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    # ✅ Clean prompt & trim to just the answer\n    caption = caption.split(\"What’s in this image?\")[-1].replace(\"Assistant:\", \"\").strip()\n    return caption\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4612d7f4-2e2c-4db4-b431-79b61e13bfa9","cell_type":"markdown","source":"### Evaluate SmolVLM Zero-Shot on Test Set\n\nIn this step, we evaluate the SmolVLM model's zero-shot captioning performance on the test set. We generate captions for each image in the test set, compare them with reference captions, and compute evaluation metrics (BLEU, METEOR, and ROUGE).\n\n1. **Generate Captions**:\n   - For each image in the test set, we load the image and attempt to generate a caption using the `zero_shot_captioning` function.\n   - If an error occurs during image processing (e.g., an invalid image file), the image is skipped, and an error message is printed.\n   - The generated captions are stored in the `predictions` list, and the reference captions are stored in the `references` list.\n\n2. **Compute Metrics**:\n   - After generating captions for all the images, we compute the evaluation metrics:\n     - **BLEU**: Measures n-gram precision.\n     - **METEOR**: Harmonic mean of precision and recall, accounting for synonyms and stemming.\n     - **ROUGE-L**: Longest common subsequence between generated and reference captions.\n\n3. **Report Results**:\n   - The computed scores (BLEU, METEOR, and ROUGE-L) are printed to give an overview of the model's performance on the test set.\n\nThis evaluation gives insight into how well the SmolVLM model performs in a zero-shot setting on the given test data.\n","metadata":{}},{"id":"d8240dfd","cell_type":"code","source":"# Step 5: Evaluate\npredictions = []\nreferences = []\n\nprint(\"Evaluating SmolVLM Zero-Shot on Test Set...\")\nfor i in tqdm(range(len(test_ds))):\n    image_path = test_ds.df.iloc[i]['filename']\n    full_path = os.path.join(test_ds.image_dir, image_path)\n\n    ref_caption = test_ds.df.iloc[i]['caption']\n\n    try:\n        image = Image.open(full_path).convert(\"RGB\")\n        gen_caption = zero_shot_captioning(image)\n    except Exception as e:\n        print(f\"[Skipped] Error processing {image_path}: {e}\")\n        continue\n\n    predictions.append(gen_caption.strip())\n    references.append([ref_caption.strip()])\n\n# Step 6: Compute metrics\nbleu_score = bleu.compute(predictions=predictions, references=references)\nmeteor_score = meteor.compute(predictions=predictions, references=references)\nrouge_score = rouge.compute(predictions=predictions, references=[r[0] for r in references])\n\n# Step 7: Report results\nprint(\"\\n=== Zero-Shot Evaluation (SmolVLM) ===\")\nprint(f\"BLEU Score:   {bleu_score['bleu']:.4f}\")\nprint(f\"METEOR Score: {meteor_score['meteor']:.4f}\")\nprint(f\"ROUGE-L:      {rouge_score['rougeL']:.4f}\")\n\n\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T09:46:40.598539Z","iopub.status.busy":"2025-04-14T09:46:40.598297Z","iopub.status.idle":"2025-04-14T10:24:42.313819Z","shell.execute_reply":"2025-04-14T10:24:42.313156Z","shell.execute_reply.started":"2025-04-14T09:46:40.598520Z"},"id":"d8240dfd","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluating SmolVLM Zero-Shot on Test Set...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 928/928 [37:58<00:00,  2.45s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","=== Zero-Shot Evaluation (SmolVLM) ===\n","BLEU Score:   0.0289\n","METEOR Score: 0.1690\n","ROUGE-L:      0.2175\n"]}],"execution_count":42},{"id":"9a076e4d-873c-428c-babc-5d9d30894ea1","cell_type":"markdown","source":"# **Part-B**\n","metadata":{}},{"id":"fb3fd0f2-c6d3-4f65-82e2-f3ac423bd36a","cell_type":"markdown","source":"### Image Occlusion Function\n\nThis function occludes a given image by masking random patches. The percentage of occlusion is controlled by the `occlusion_pct` parameter, and the patch size is controlled by the `patch_size` parameter.\n\n1. **Function Overview**:\n   - The `occlude_image` function takes an image (`img`), an occlusion percentage (`occlusion_pct`), and an optional patch size (`patch_size`).\n   - The image is first converted to a NumPy array for easier manipulation.\n   - The image dimensions are checked to ensure that they are divisible by the patch size.\n   - A number of patches to mask is calculated based on the `occlusion_pct`.\n   - Random patches are selected and set to black (occluded).\n\n2. **Steps**:\n   - The image is divided into non-overlapping patches of the specified size (`patch_size`).\n   - A set number of patches is randomly chosen to be masked (set to black).\n   - The function returns the occluded image as a PIL image.\n\nThis function can be useful for tasks like testing robustness to occlusion or generating perturbations for training.\n","metadata":{}},{"id":"629f7ac7","cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nimport random\n\ndef occlude_image(img: Image.Image, occlusion_pct: float, patch_size: int = 16) -> Image.Image:\n    img_np = np.array(img)\n    h, w, _ = img_np.shape\n    assert h % patch_size == 0 and w % patch_size == 0, \"Image must be divisible by patch size.\"\n\n    num_patches = (h // patch_size) * (w // patch_size)\n    num_mask = int(occlusion_pct * num_patches)\n\n    patch_coords = [(i, j) for i in range(h // patch_size) for j in range(w // patch_size)]\n    masked_coords = random.sample(patch_coords, num_mask)\n\n    for i, j in masked_coords:\n        y1, y2 = i * patch_size, (i + 1) * patch_size\n        x1, x2 = j * patch_size, (j + 1) * patch_size\n        img_np[y1:y2, x1:x2] = 0  # black patch\n\n    return Image.fromarray(img_np)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2025-04-14T07:34:44.989001Z","iopub.status.busy":"2025-04-14T07:34:44.988605Z","iopub.status.idle":"2025-04-14T07:34:44.994797Z","shell.execute_reply":"2025-04-14T07:34:44.994104Z","shell.execute_reply.started":"2025-04-14T07:34:44.988975Z"},"id":"629f7ac7","outputId":"587349c4-b93c-4f0f-ca8d-374ab5de5a2f","trusted":true},"outputs":[],"execution_count":35},{"id":"8978290f-9cd7-4dab-85c6-7d66538e8e3c","cell_type":"markdown","source":"### Evaluate on Occluded Images\n\nThis function evaluates a model on images with varying levels of occlusion. It applies occlusion to each image in the test dataset, generates a caption using the model, and tracks the results.\n\n1. **Function Overview**:\n   - The `evaluate_on_occluded_images` function takes the following inputs:\n     - `model_type`: Specifies the model being used (e.g., \"smolvlm\").\n     - `occlusion_pct`: The percentage of the image to occlude (e.g., 0.2 for 20% occlusion).\n     - `test_df`: The test dataframe containing image filenames and reference captions.\n     - `image_dir`: The directory containing the images.\n   - The function loops through each image in the test dataset, applies occlusion, and generates a caption.\n   - It tracks the predictions, references, occlusion levels, and image indices for later analysis.\n\n2. **Steps**:\n   - For each image:\n     - The image is loaded and resized.\n     - Occlusion is applied using the `occlude_image` function.\n     - The model is used to generate a caption for the occluded image.\n     - The generated caption, along with the reference caption and other relevant data, is stored for evaluation.\n   - If the model type is \"smolvlm\", the `zero_shot_captioning` function is used; otherwise, the occluded image is saved temporarily, and the model generates a caption using that file.\n\n3. **Outputs**:\n   - The function returns four lists:\n     - `predictions`: The captions generated by the model.\n     - `references`: The reference captions for the images.\n     - `levels`: The occlusion percentage applied to each image.\n     - `indices`: The indices of the images in the test set (to track which samples were processed).\n\nThis function is useful for evaluating how well the model performs under various occlusion levels, which can provide insight into the model’s robustness to partial information.\n","metadata":{}},{"id":"TnfToRFMK1bQ","cell_type":"code","source":"import tempfile\n\ndef evaluate_on_occluded_images(model_type: str, occlusion_pct: float, test_df, image_dir):\n    predictions, references, levels, indices = [], [], [], []\n\n    for i in tqdm(range(len(test_df)), desc=f\"{model_type} - Occlusion {int(occlusion_pct*100)}%\"):\n        row = test_df.iloc[i]\n        img_path = os.path.join(image_dir, row['filename'])\n        ref_caption = row['caption'].strip()\n\n        img = Image.open(img_path).convert(\"RGB\").resize((256, 256))  # Resize for patching\n        occluded_img = occlude_image(img, occlusion_pct=occlusion_pct)\n\n        if model_type == \"smolvlm\":\n            try:\n                gen_caption = zero_shot_captioning(occluded_img)\n            except Exception as e:\n                print(f\"Error on image {img_path}: {e}\")\n                continue\n        else:\n            with tempfile.NamedTemporaryFile(suffix=\".png\") as tmpfile:# Save occluded image to a temporary file and pass its path\n                occluded_img.save(tmpfile.name)\n                gen_caption = trainer.generate_caption(tmpfile.name)\n\n        predictions.append(gen_caption.strip())\n        references.append([ref_caption])\n        levels.append(int(occlusion_pct * 100))\n        indices.append(i)  # ✅ Track which sample this was\n\n    return predictions, references, levels, indices\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T07:34:49.741644Z","iopub.status.busy":"2025-04-14T07:34:49.741081Z","iopub.status.idle":"2025-04-14T07:34:49.748140Z","shell.execute_reply":"2025-04-14T07:34:49.747295Z","shell.execute_reply.started":"2025-04-14T07:34:49.741620Z"},"id":"TnfToRFMK1bQ","trusted":true},"outputs":[],"execution_count":null},{"id":"955381ac-4e4a-48fa-b804-86e9b8ceb7d0","cell_type":"markdown","source":"### Compute Evaluation Metrics\n\nThis section defines a utility function to compute standard evaluation metrics for image captioning: BLEU, METEOR, and ROUGE-L.\n\n1. **Metric Initialization**:\n   - The evaluation metrics are loaded using the `evaluate` library:\n     - **BLEU**: Measures n-gram precision between the generated and reference captions.\n     - **METEOR**: Considers precision, recall, stemming, and synonymy for a more nuanced evaluation.\n     - **ROUGE-L**: Measures the longest common subsequence between the prediction and reference.\n\n2. **`compute_metrics` Function**:\n   - Takes in:\n     - `predictions`: A list of generated captions.\n     - `references`: A list of corresponding reference captions (in nested list format).\n   - Computes each metric using the loaded evaluation tools.\n   - Returns the scores for BLEU, METEOR, and ROUGE-L as floats.\n\nThis function serves as a centralized way to evaluate and compare captioning model performance across different scenarios or perturbations.\n","metadata":{}},{"id":"950f9c9a-c111-44fe-8f90-0c59e2b407ec","cell_type":"code","source":"import evaluate\n\nbleu = evaluate.load(\"bleu\")\nmeteor = evaluate.load(\"meteor\")\nrouge = evaluate.load(\"rouge\")\n\ndef compute_metrics(predictions, references):\n    bleu_score = bleu.compute(predictions=predictions, references=references)[\"bleu\"]\n    meteor_score = meteor.compute(predictions=predictions, references=references)[\"meteor\"]\n    rouge_score = rouge.compute(predictions=predictions, references=[r[0] for r in references])[\"rougeL\"]\n    return bleu_score, meteor_score, rouge_score\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"616b9fd3-2a5e-4d95-8ede-d9aeb91a84c0","cell_type":"markdown","source":"### Evaluate Models at Multiple Occlusion Levels\n\nThis section runs a comparative evaluation of two models—`custom` and `smolvlm`—under various levels of image occlusion. It computes captioning performance metrics and prepares a dataset for further analysis in Part C.\n\n1. **Setup**:\n   - A list of occlusion levels (`10%`, `50%`, `80%`) is defined.\n   - `results` is used to store aggregated metric scores.\n   - `full_outputs` stores detailed per-sample results for later use in Part C.\n\n2. **Evaluation Loop**:\n   - For each model (`custom` and `smolvlm`) and each occlusion level:\n     - Occluded images are evaluated using `evaluate_on_occluded_images`.\n     - Metrics (BLEU, METEOR, ROUGE-L) are computed using `compute_metrics`.\n     - The results are appended to the `results` list.\n     - Each prediction, along with metadata (filename, occlusion level, model type, original and generated captions), is stored in `full_outputs`.\n\n3. **Export Results for Part C**:\n   - The `full_outputs` list is converted into a DataFrame.\n   - The data is saved as a CSV file named `occlusion_eval_partC.csv`, which can be used for training or evaluating a classifier in Part C.\n\nThis block enables systematic robustness evaluation of captioning models under occlusion and prepares labeled data for downstream analysis or classification tasks.\n","metadata":{}},{"id":"31c668e6-b348-45b7-8b3b-9888887c5790","cell_type":"code","source":"import pandas as pd\n\nocclusion_levels = [0.1, 0.5, 0.8]\nresults = []\nfull_outputs = []  # ✅ For Part C\n\nfor model_type in [\"custom\",\"smolvlm\"]:\n    for level in occlusion_levels:\n        preds, refs, levels, indices = evaluate_on_occluded_images(\n            model_type=model_type,\n            occlusion_pct=level,\n            test_df=test_ds.df,\n            image_dir=test_ds.image_dir\n        )\n        bleu_score, meteor_score, rouge_score = compute_metrics(preds, refs)\n\n        results.append({\n            \"model\": model_type,\n            \"occlusion_level\": int(level * 100),\n            \"BLEU\": bleu_score,\n            \"METEOR\": meteor_score,\n            \"ROUGE-L\": rouge_score,\n        })\n\n        # ✅ Save full per-sample data for Part C using correct indices\n        for i, idx in enumerate(indices):\n            full_outputs.append({\n                \"filename\": test_ds.df.iloc[idx][\"filename\"],\n                \"perturbation_level\": int(level * 100),\n                \"original_caption\": refs[i][0],\n                \"generated_caption\": preds[i],\n                \"model\": model_type\n            })\n\n# ✅ After the loop: save all data to CSV\ndf_full = pd.DataFrame(full_outputs)\ndf_full.to_csv(\"occlusion_eval_partC.csv\", index=False)","metadata":{"execution":{"iopub.execute_input":"2025-04-14T07:35:02.431556Z","iopub.status.busy":"2025-04-14T07:35:02.430969Z","iopub.status.idle":"2025-04-14T09:42:37.109279Z","shell.execute_reply":"2025-04-14T09:42:37.108742Z","shell.execute_reply.started":"2025-04-14T07:35:02.431533Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["custom - Occlusion 10%: 100%|██████████| 928/928 [10:52<00:00,  1.42it/s]\n","custom - Occlusion 50%: 100%|██████████| 928/928 [10:35<00:00,  1.46it/s]\n","custom - Occlusion 80%: 100%|██████████| 928/928 [10:54<00:00,  1.42it/s]\n","smolvlm - Occlusion 10%: 100%|██████████| 928/928 [36:59<00:00,  2.39s/it]\n","smolvlm - Occlusion 50%: 100%|██████████| 928/928 [29:18<00:00,  1.90s/it]\n","smolvlm - Occlusion 80%: 100%|██████████| 928/928 [28:33<00:00,  1.85s/it]\n"]}],"execution_count":null},{"id":"7cf2165e-c81f-43fb-83dc-b734f467c246","cell_type":"markdown","source":"### Print Robustness Summary\n\nThis section displays a formatted summary of the model performance across different occlusion levels.\n\n1. **Loop Through Results**:\n   - Iterates over each entry in the `results` list (which contains performance metrics for both `custom` and `smolvlm` models at various occlusion levels).\n   \n2. **Formatted Output**:\n   - For each result, prints the model name, occlusion level, and its corresponding evaluation scores:\n     - **BLEU**\n     - **METEOR**\n     - **ROUGE-L**\n\nThis summary gives a quick and clear view of how robust each model is when faced with increasing levels of image occlusion.\n","metadata":{}},{"id":"4e0d09ce-823b-447c-95db-a0b6639576a7","cell_type":"code","source":"print(\"\\n=== Robustness Summary ===\")\nfor res in results:\n    print(f\"{res['model']} | Occlusion {res['occlusion_level']}% -> \"\n          f\"BLEU: {res['BLEU']:.4f}, METEOR: {res['METEOR']:.4f}, ROUGE-L: {res['ROUGE-L']:.4f}\")\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T09:43:25.950214Z","iopub.status.busy":"2025-04-14T09:43:25.949956Z","iopub.status.idle":"2025-04-14T09:43:25.954748Z","shell.execute_reply":"2025-04-14T09:43:25.953948Z","shell.execute_reply.started":"2025-04-14T09:43:25.950198Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","=== Robustness Summary ===\n","custom | Occlusion 10% -> BLEU: 0.0012, METEOR: 0.1200, ROUGE-L: 0.1498\n","custom | Occlusion 50% -> BLEU: 0.0000, METEOR: 0.1184, ROUGE-L: 0.1487\n","custom | Occlusion 80% -> BLEU: 0.0000, METEOR: 0.1207, ROUGE-L: 0.1505\n","smolvlm | Occlusion 10% -> BLEU: 0.0157, METEOR: 0.1399, ROUGE-L: 0.1974\n","smolvlm | Occlusion 50% -> BLEU: 0.0011, METEOR: 0.0793, ROUGE-L: 0.1453\n","smolvlm | Occlusion 80% -> BLEU: 0.0002, METEOR: 0.0529, ROUGE-L: 0.0963\n"]}],"execution_count":39}]}