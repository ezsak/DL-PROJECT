{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11403845,"sourceType":"datasetVersion","datasetId":7142935}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Part-C**\n","metadata":{}},{"cell_type":"markdown","source":"### Imports for Part C: Caption Source Classification\n\nThis section sets up the environment for building a classifier to distinguish between captions generated by different models (e.g., `custom` vs `smolvlm`).\n\n1. **Core Libraries**:\n   - `os`, `pandas`: File handling and data manipulation.\n   - `torch`, `nn`, `DataLoader`, `Dataset`: PyTorch tools for defining and training the classification model.\n\n2. **Transformers**:\n   - `BertTokenizer`, `BertModel`: Tokenizer and model components from HuggingFace's BERT.\n   - `get_linear_schedule_with_warmup`: Learning rate scheduler for stable training.\n\n3. **Optimization**:\n   - `AdamW`: Weight-decay variant of Adam optimizer from `torch.optim`.\n\n4. **Evaluation & Splitting**:\n   - `train_test_split` from scikit-learn to split the dataset.\n   - `accuracy_score`, `classification_report` for performance evaluation.\n\nThese tools are essential for implementing a BERT-based classifier to identify the source of a caption based on its textual content.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW  # ← AdamW is now from torch.optim\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-04-14T14:53:03.309552Z","iopub.status.busy":"2025-04-14T14:53:03.309187Z","iopub.status.idle":"2025-04-14T14:53:03.313997Z","shell.execute_reply":"2025-04-14T14:53:03.313185Z","shell.execute_reply.started":"2025-04-14T14:53:03.309532Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"### Load and Prepare Caption Classification Data\n\nThis function preprocesses the caption dataset for training a classifier that distinguishes between captions generated by different models.\n\n1. **Read CSV File**:\n   - Loads the dataset from a CSV file using `pandas`.\n\n2. **Clean the Data**:\n   - Removes rows with missing values in critical columns such as:\n     - `original_caption`\n     - `generated_caption`\n     - `perturbation_level`\n     - `model`\n     - `filename`\n\n3. **Text Formatting**:\n   - Ensures captions and perturbation levels are treated as strings.\n   - Concatenates `original_caption`, `generated_caption`, and `perturbation_level` into a single string using `<SEP>` as a delimiter.\n   - This combined string serves as the input for BERT.\n\n4. **Label Encoding**:\n   - Creates a binary label:\n     - `0` for `smolvlm`\n     - `1` for `custom` (or other models)\n\n5. **Return**:\n   - A cleaned and formatted DataFrame with three columns:\n     - `filename`: Image file name (for tracking)\n     - `input_text`: Combined input for classification\n     - `label`: Ground truth label indicating the source model\n\nThis function is essential for preparing the dataset for a BERT-based binary classification task in Part C.\n","metadata":{}},{"cell_type":"code","source":"def load_caption_data(data_file):\n    df = pd.read_csv(data_file)\n\n    # Drop rows with missing values in relevant columns\n    df = df.dropna(subset=[\"original_caption\", \"generated_caption\", \"perturbation_level\", \"model\", \"filename\"])\n    \n    # Ensure all are strings and format input text\n    df[\"original_caption\"] = df[\"original_caption\"].astype(str)\n    df[\"generated_caption\"] = df[\"generated_caption\"].astype(str)\n    df[\"perturbation_level\"] = df[\"perturbation_level\"].astype(str)\n\n    df[\"input_text\"] = df[\"original_caption\"] + \" <SEP> \" + df[\"generated_caption\"] + \" <SEP> \" + df[\"perturbation_level\"]\n\n    # Create binary labels\n    df[\"label\"] = df[\"model\"].apply(lambda x: 0 if x.lower() == \"smolvlm\" else 1)\n\n    return df[[\"filename\", \"input_text\", \"label\"]]\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T14:53:03.315293Z","iopub.status.busy":"2025-04-14T14:53:03.315091Z","iopub.status.idle":"2025-04-14T14:53:03.334150Z","shell.execute_reply":"2025-04-14T14:53:03.333420Z","shell.execute_reply.started":"2025-04-14T14:53:03.315277Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### CaptionClassificationDataset: Custom Dataset for BERT Classifier\n\nThis class defines a custom PyTorch `Dataset` for training a BERT-based classifier on caption comparison data.\n\n1. **Initialization (`__init__`)**:\n   - Accepts a list of input texts, corresponding labels, a tokenizer, and a maximum sequence length.\n   - Stores these for use in batching and tokenization.\n\n2. **Length (`__len__`)**:\n   - Returns the number of samples in the dataset.\n\n3. **Item Access (`__getitem__`)**:\n   - Tokenizes the input text at the given index.\n   - Applies truncation and padding to a fixed maximum length (`max_length`).\n   - Returns a dictionary with:\n     - `input_ids`: Encoded token IDs.\n     - `attention_mask`: Mask for non-padding tokens.\n     - `label`: Binary label as a PyTorch tensor.\n\nThis dataset is designed to be used with a `DataLoader` for efficient batching during training and evaluation of the BERT classifier in Part C.\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass CaptionClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            return_tensors=\"pt\",\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True\n        )\n        return {\n            'input_ids': encoding[\"input_ids\"].squeeze(),\n            'attention_mask': encoding[\"attention_mask\"].squeeze(),\n            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T14:53:03.335071Z","iopub.status.busy":"2025-04-14T14:53:03.334878Z","iopub.status.idle":"2025-04-14T14:53:03.349869Z","shell.execute_reply":"2025-04-14T14:53:03.349297Z","shell.execute_reply.started":"2025-04-14T14:53:03.335058Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"### CaptionClassifier: BERT-Based Binary Classification Model\n\nThis class defines a neural network for classifying whether a given caption pair was generated by SmolVLM or a custom model.\n\n1. **Initialization (`__init__`)**:\n   - Loads a pretrained BERT model using the specified model name (e.g., `bert-base-uncased`).\n   - Adds a dropout layer (`0.1` probability) to prevent overfitting.\n   - Includes a fully connected layer (`nn.Linear`) that maps the BERT output to the desired number of classes (`num_classes`, typically 2).\n\n2. **Forward Pass (`forward`)**:\n   - Takes `input_ids` and `attention_mask` as input.\n   - Extracts the pooled output (`[CLS]` token representation) from the BERT model.\n   - Applies dropout and feeds the result through the linear layer to produce logits for classification.\n\nThis model is the core of the binary classifier used in Part C to determine which model generated a given caption.\n","metadata":{}},{"cell_type":"code","source":"class CaptionClassifier(nn.Module):\n    def __init__(self, bert_model_name, num_classes):\n        super(CaptionClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(bert_model_name)\n        self.dropout = nn.Dropout(0.1)\n        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):  # ← make sure this is indented correctly\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        x = self.dropout(pooled_output)\n        logits = self.fc(x)\n        return logits\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T14:53:03.351256Z","iopub.status.busy":"2025-04-14T14:53:03.351059Z","iopub.status.idle":"2025-04-14T14:53:03.367760Z","shell.execute_reply":"2025-04-14T14:53:03.367003Z","shell.execute_reply.started":"2025-04-14T14:53:03.351242Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### train_classifier: Training Loop for the BERT Classifier\n\nThis function performs one training epoch over the classification dataset.\n\n1. **Model in Training Mode**:\n   - Sets the model to training mode using `model.train()` to enable dropout and gradient tracking.\n\n2. **Batch Iteration**:\n   - Iterates through the DataLoader, processing one batch at a time.\n\n3. **Gradient Reset**:\n   - Clears old gradients with `optimizer.zero_grad()`.\n\n4. **Move Data to Device**:\n   - Transfers input IDs, attention masks, and labels to the specified device (CPU or GPU).\n\n5. **Forward Pass**:\n   - Computes logits by passing the inputs through the model.\n\n6. **Loss Calculation**:\n   - Uses `CrossEntropyLoss` to compute the classification loss between predicted logits and ground truth labels.\n\n7. **Backward Pass and Optimization**:\n   - Performs backpropagation with `loss.backward()`.\n   - Updates model weights using the optimizer.\n   - Steps the learning rate scheduler.\n\nThis function is part of the training loop used to fine-tune the BERT classifier in Part C.\n","metadata":{}},{"cell_type":"code","source":"def train_classifier(model, data_loader, optimizer, scheduler, device):\n    model.train()\n    for batch in data_loader:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        loss = nn.CrossEntropyLoss()(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()","metadata":{"execution":{"iopub.execute_input":"2025-04-14T14:53:03.368690Z","iopub.status.busy":"2025-04-14T14:53:03.368425Z","iopub.status.idle":"2025-04-14T14:53:03.386726Z","shell.execute_reply":"2025-04-14T14:53:03.386085Z","shell.execute_reply.started":"2025-04-14T14:53:03.368670Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### evaluate_classifier: Evaluation Function for the BERT Classifier\n\nThis function evaluates the performance of the classifier on a given dataset.\n\n1. **Evaluation Mode**:\n   - Sets the model to evaluation mode with `model.eval()` to disable dropout and gradient computation.\n\n2. **Batch Iteration (No Gradients)**:\n   - Loops through each batch in the DataLoader using `torch.no_grad()` to prevent gradient calculations.\n\n3. **Move Data to Device**:\n   - Sends inputs and labels to the appropriate device (CPU or GPU).\n\n4. **Prediction**:\n   - Computes logits with the model and uses `torch.max` to get the predicted class labels.\n\n5. **Collect Predictions and Labels**:\n   - Aggregates the predictions and actual labels for the entire dataset.\n\n6. **Compute Metrics**:\n   - Uses `classification_report` from `sklearn` to calculate precision, recall, F1-score, etc.\n   - Returns both the accuracy score and the full classification report as a dictionary.\n\nThis function is used in Part C to evaluate how well the classifier can distinguish between captions generated by SmolVLM and the custom model.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ndef evaluate_classifier(model, data_loader, device):\n    model.eval()\n    predictions = []\n    actual_labels = []\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            predictions.extend(preds.cpu().tolist())\n            actual_labels.extend(labels.cpu().tolist())\n    \n    # Get report as dict for macro scores\n    report_dict = classification_report(actual_labels, predictions, output_dict=True)\n    return accuracy_score(actual_labels, predictions), report_dict\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T14:53:03.387925Z","iopub.status.busy":"2025-04-14T14:53:03.387613Z","iopub.status.idle":"2025-04-14T14:53:03.401278Z","shell.execute_reply":"2025-04-14T14:53:03.400621Z","shell.execute_reply.started":"2025-04-14T14:53:03.387909Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### predict_model_source: Predicts the Model Source Based on Captions\n\nThis function takes an original caption, a generated caption, and a perturbation level to predict whether the caption was generated by SmolVLM or a custom model.\n\n1. **Evaluation Mode**:\n   - Sets the model to evaluation mode using `model.eval()` to ensure no gradients are computed during prediction.\n\n2. **Input Construction**:\n   - Constructs the input text by concatenating the original caption, generated caption, and perturbation level with a separator (`<SEP>`).\n\n3. **Tokenization**:\n   - Tokenizes the input text using the provided tokenizer with padding and truncation to ensure consistent input length.\n\n4. **Prediction**:\n   - Passes the tokenized inputs through the model to obtain logits.\n   - Applies `torch.max` to extract the predicted class (0 for SmolVLM, 1 for Custom).\n\n5. **Interpretation**:\n   - Interprets the model's prediction, returning a label for the corresponding model source:\n     - `\"Model A (SmolVLM)\"` if the prediction is 0.\n     - `\"Model B (Custom)\"` if the prediction is 1.\n\nThis function is used in Part C to classify whether the generated caption comes from SmolVLM or the custom model based on the provided captions and perturbation levels.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_model_source(original_caption, generated_caption, perturbation_level, model, tokenizer, device, max_length=128):\n    model.eval()\n    \n    # Construct input text in the required format\n    input_text = f\"{original_caption} <SEP> {generated_caption} <SEP> {perturbation_level}\"\n    \n    # Tokenize\n    encoding = tokenizer(input_text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    # Predict\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n    \n    # Interpret prediction\n    return \"Model A (SmolVLM)\" if preds.item() == 0 else \"Model B (Custom)\"\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T14:53:03.517084Z","iopub.status.busy":"2025-04-14T14:53:03.516789Z","iopub.status.idle":"2025-04-14T14:53:03.522259Z","shell.execute_reply":"2025-04-14T14:53:03.521683Z","shell.execute_reply.started":"2025-04-14T14:53:03.517062Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"### Parameter Setup\n\nThe following parameters are configured for training the BERT-based caption classifier:\n\n1. **BERT Model Name**:\n   - `bert-base-uncased`: The BERT model used for encoding the input text. It is pre-trained on a large corpus of text and is case-insensitive (`uncased`).\n\n2. **Number of Classes**:\n   - `2`: The model will classify captions into two categories: \"SmolVLM\" and \"Custom.\"\n\n3. **Maximum Input Length**:\n   - `128`: The maximum length of the input sequence after tokenization. Longer sequences will be truncated, and shorter ones will be padded.\n\n4. **Batch Size**:\n   - `16`: The number of samples processed together in each forward/backward pass.\n\n5. **Number of Epochs**:\n   - `4`: The number of times the entire dataset will be passed through the model during training.\n\n6. **Learning Rate**:\n   - `2e-5`: The learning rate for the optimizer. It controls how quickly the model updates its parameters during training.\n\nThese parameters are set to ensure efficient training while preventing overfitting and underfitting.\n","metadata":{}},{"cell_type":"code","source":"# Set up parameters\nbert_model_name = 'bert-base-uncased'\nnum_classes = 2\nmax_length = 128\nbatch_size = 16\nnum_epochs = 4\nlearning_rate = 2e-5","metadata":{"execution":{"iopub.execute_input":"2025-04-14T14:53:03.523412Z","iopub.status.busy":"2025-04-14T14:53:03.523182Z","iopub.status.idle":"2025-04-14T14:53:03.540361Z","shell.execute_reply":"2025-04-14T14:53:03.539673Z","shell.execute_reply.started":"2025-04-14T14:53:03.523393Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"### split_caption_data_by_filename: Split the DataFrame into Train, Validation, and Test Sets\n\nThis function splits the input DataFrame into three sets: training, validation, and testing. The split is done based on unique filenames to ensure no overlap between the sets.\n\n1. **Input Parameters**:\n   - `df`: The DataFrame containing the data with a column `filename` for the image filenames.\n   - `train_ratio`: Proportion of data used for training (default is 0.7).\n   - `val_ratio`: Proportion of data used for validation (default is 0.1).\n   - `test_ratio`: Proportion of data used for testing (default is 0.2).\n   - `random_state`: Seed for random splitting (default is 42).\n\n2. **Assertions**:\n   - Checks that the sum of the train, validation, and test ratios is exactly 1.0.\n\n3. **Step 1: Train vs Temp Split**:\n   - Splits the unique filenames into `train_filenames` (for training) and `temp_filenames` (for validation and testing combined).\n\n4. **Step 2: Val vs Test Split**:\n   - Further splits the `temp_filenames` into `val_filenames` and `test_filenames` based on the specified validation and testing ratios.\n\n5. **Step 3: Filter the DataFrame**:\n   - Filters the main DataFrame to create separate DataFrames for training, validation, and testing by matching the filenames.\n\n6. **Returns**:\n   - The function returns three DataFrames: `train_df`, `val_df`, and `test_df`.\n\nThis function is helpful for preparing the dataset into distinct sets for model training and evaluation, ensuring no overlap between them.\n","metadata":{}},{"cell_type":"code","source":"def split_caption_data_by_filename(df, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2, random_state=42):\n    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-5, \"Ratios must sum to 1.0\"\n    \n    # Unique filenames\n    unique_filenames = df['filename'].unique()\n    \n    # First split: Train vs Temp (Val + Test)\n    train_filenames, temp_filenames = train_test_split(\n        unique_filenames,\n        test_size=(1 - train_ratio),\n        random_state=random_state\n    )\n    \n    # Second split: Val vs Test\n    val_ratio_adjusted = val_ratio / (val_ratio + test_ratio)\n    val_filenames, test_filenames = train_test_split(\n        temp_filenames,\n        test_size=(1 - val_ratio_adjusted),\n        random_state=random_state\n    )\n\n    # Now filter the main DataFrame by filenames\n    train_df = df[df['filename'].isin(train_filenames)]\n    val_df = df[df['filename'].isin(val_filenames)]\n    test_df = df[df['filename'].isin(test_filenames)]\n\n    return train_df, val_df, test_df\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T14:53:03.541168Z","iopub.status.busy":"2025-04-14T14:53:03.540974Z","iopub.status.idle":"2025-04-14T14:53:03.556035Z","shell.execute_reply":"2025-04-14T14:53:03.555320Z","shell.execute_reply.started":"2025-04-14T14:53:03.541154Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"### Loading and Splitting the Data\n\nThis section demonstrates how to load the caption data and split it into training, validation, and test sets for model evaluation.\n\n1. **Loading the Data**:\n   - The data is loaded from a CSV file (`occlusion_eval_partC.csv`) using the `load_caption_data` function, which processes the file and prepares it for training.\n\n2. **Splitting the Data**:\n   - The `split_caption_data_by_filename` function is used to split the dataset into three sets:\n     - **Training Set**: 70% of the data.\n     - **Validation Set**: 10% of the data.\n     - **Test Set**: 20% of the data.\n   - The split is done based on unique filenames to ensure no overlap between the sets.\n\n3. **Extracting Input Texts and Labels**:\n   - From the split DataFrames (`train_df`, `val_df`, and `test_df`), the input text (`input_text`) and corresponding labels (`label`) are extracted and stored in separate lists:\n     - `train_texts`, `train_labels`\n     - `val_texts`, `val_labels`\n     - `test_texts`, `test_labels`\n   \nThis preparation is necessary before training or evaluating a model, ensuring the data is cleanly partitioned for proper validation.\n","metadata":{}},{"cell_type":"code","source":"# Load your data\ndata_file = \"/kaggle/input/datacsv/occlusion_eval_partC.csv\"\ndf = load_caption_data(data_file)\n\n# Split by image filenames\ntrain_df, val_df, test_df = split_caption_data_by_filename(df)\n\n# Extract input_text and labels for each split\ntrain_texts = train_df[\"input_text\"].tolist()\ntrain_labels = train_df[\"label\"].tolist()\n\nval_texts = val_df[\"input_text\"].tolist()\nval_labels = val_df[\"label\"].tolist()\n\ntest_texts = test_df[\"input_text\"].tolist()\ntest_labels = test_df[\"label\"].tolist()\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T14:53:03.557698Z","iopub.status.busy":"2025-04-14T14:53:03.557472Z","iopub.status.idle":"2025-04-14T14:53:03.625079Z","shell.execute_reply":"2025-04-14T14:53:03.624375Z","shell.execute_reply.started":"2025-04-14T14:53:03.557682Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"### Model Training and Evaluation\n\nThis section covers the setup, training, and evaluation of a BERT-based classifier for distinguishing between captions generated by SmolVLM and a custom model.\n\n1. **Tokenizer Initialization**:\n   - A BERT tokenizer is initialized using the `BertTokenizer.from_pretrained` method to preprocess the input text for the classifier.\n\n2. **Dataset Creation**:\n   - The `CaptionClassificationDataset` class is used to create datasets for the training and validation splits. This class handles tokenization and transformation of the input text into the required format for BERT.\n\n3. **DataLoader Creation**:\n   - DataLoaders are created for both training and validation datasets, allowing efficient batching and shuffling during training.\n\n4. **Device and Model Setup**:\n   - The model is moved to the appropriate device (`cuda` if available, otherwise `cpu`). The `CaptionClassifier` model is instantiated with a BERT backbone, configured for binary classification.\n\n5. **Optimizer and Scheduler**:\n   - The optimizer is set to `AdamW`, and a learning rate scheduler is initialized with linear warmup, ensuring a smooth training process.\n\n6. **Training Loop**:\n   - The training loop iterates over multiple epochs, training the classifier on the training dataset. After each epoch, the model is evaluated on the validation dataset, and accuracy and detailed classification reports are printed.\n\n7. **Model Saving**:\n   - After training, the model’s state_dict is saved to a file (`bert_caption_classifier.pth`) for later use.\n\nThis procedure ensures that the model is trained to classify captions generated by different models and can be evaluated on validation data for performance.\n","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW \n\n# Initialize tokenizer\ntokenizer = BertTokenizer.from_pretrained(bert_model_name)\n\n# Create datasets\ntrain_dataset = CaptionClassificationDataset(train_texts, train_labels, tokenizer, max_length)\nval_dataset = CaptionClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n\n# Create dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n\n# Set up device and model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CaptionClassifier(bert_model_name, num_classes).to(device)\n\n# Optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=learning_rate)\ntotal_steps = len(train_dataloader) * num_epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n# Training loop\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n    train_classifier(model, train_dataloader, optimizer, scheduler, device)\n    accuracy, report = evaluate_classifier(model, val_dataloader, device)\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n    print(report)\n\n# Save the model\ntorch.save(model.state_dict(), \"bert_caption_classifier.pth\")\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T14:53:03.626164Z","iopub.status.busy":"2025-04-14T14:53:03.625911Z","iopub.status.idle":"2025-04-14T14:56:27.676613Z","shell.execute_reply":"2025-04-14T14:56:27.676007Z","shell.execute_reply.started":"2025-04-14T14:53:03.626144Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/4\n","Validation Accuracy: 0.9762\n","{'0': {'precision': 0.9550173010380623, 'recall': 1.0, 'f1-score': 0.9769911504424779, 'support': 276}, '1': {'precision': 1.0, 'recall': 0.9518518518518518, 'f1-score': 0.9753320683111955, 'support': 270}, 'accuracy': 0.9761904761904762, 'macro avg': {'precision': 0.9775086505190311, 'recall': 0.9759259259259259, 'f1-score': 0.9761616093768366, 'support': 546}, 'weighted avg': {'precision': 0.9772614928324272, 'recall': 0.9761904761904762, 'f1-score': 0.9761707252127227, 'support': 546}}\n","Epoch 2/4\n","Validation Accuracy: 0.9762\n","{'0': {'precision': 0.9550173010380623, 'recall': 1.0, 'f1-score': 0.9769911504424779, 'support': 276}, '1': {'precision': 1.0, 'recall': 0.9518518518518518, 'f1-score': 0.9753320683111955, 'support': 270}, 'accuracy': 0.9761904761904762, 'macro avg': {'precision': 0.9775086505190311, 'recall': 0.9759259259259259, 'f1-score': 0.9761616093768366, 'support': 546}, 'weighted avg': {'precision': 0.9772614928324272, 'recall': 0.9761904761904762, 'f1-score': 0.9761707252127227, 'support': 546}}\n","Epoch 3/4\n","Validation Accuracy: 0.9762\n","{'0': {'precision': 0.9550173010380623, 'recall': 1.0, 'f1-score': 0.9769911504424779, 'support': 276}, '1': {'precision': 1.0, 'recall': 0.9518518518518518, 'f1-score': 0.9753320683111955, 'support': 270}, 'accuracy': 0.9761904761904762, 'macro avg': {'precision': 0.9775086505190311, 'recall': 0.9759259259259259, 'f1-score': 0.9761616093768366, 'support': 546}, 'weighted avg': {'precision': 0.9772614928324272, 'recall': 0.9761904761904762, 'f1-score': 0.9761707252127227, 'support': 546}}\n","Epoch 4/4\n","Validation Accuracy: 0.9762\n","{'0': {'precision': 0.9646643109540636, 'recall': 0.9891304347826086, 'f1-score': 0.9767441860465116, 'support': 276}, '1': {'precision': 0.9885931558935361, 'recall': 0.9629629629629629, 'f1-score': 0.9756097560975611, 'support': 270}, 'accuracy': 0.9761904761904762, 'macro avg': {'precision': 0.9766287334237999, 'recall': 0.9760466988727858, 'f1-score': 0.9761769710720363, 'support': 546}, 'weighted avg': {'precision': 0.9764972562538027, 'recall': 0.9761904761904762, 'f1-score': 0.9761832042036238, 'support': 546}}\n"]}],"execution_count":null},{"cell_type":"markdown","source":"### Model Evaluation on Test Set\n\nThis section evaluates the trained BERT classifier on the test set, reporting the accuracy and detailed classification metrics.\n\n1. **Test Dataset Setup**:\n   - The test dataset is created using the `CaptionClassificationDataset` class, where the `test_texts` and `test_labels` are tokenized and formatted for input into the BERT model.\n   \n2. **Test DataLoader Creation**:\n   - A `DataLoader` is initialized for the test dataset, allowing the model to efficiently process the data in batches.\n\n3. **Test Evaluation**:\n   - The model is evaluated on the test dataset using the `evaluate_classifier` function, which computes the accuracy and generates a detailed classification report. This function returns the accuracy score along with the full classification report, which includes precision, recall, and F1-score metrics for each class.\n\n4. **Results Display**:\n   - The test accuracy and classification report are printed for analysis, allowing for an assessment of how well the classifier generalizes to unseen data.\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Prepare test set\ntest_dataset = CaptionClassificationDataset(test_texts, test_labels, tokenizer, max_length)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\n# evaluate_classifier on test set\ntest_accuracy, test_report = evaluate_classifier(model, test_dataloader, device)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(\"Classification Report on Test Set:\")\nprint(test_report)\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T14:56:27.677730Z","iopub.status.busy":"2025-04-14T14:56:27.677462Z","iopub.status.idle":"2025-04-14T14:56:33.008402Z","shell.execute_reply":"2025-04-14T14:56:33.007485Z","shell.execute_reply.started":"2025-04-14T14:56:27.677706Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.9700\n","Classification Report on Test Set:\n","{'0': {'precision': 0.9615384615384616, 'recall': 0.9803921568627451, 'f1-score': 0.970873786407767, 'support': 561}, '1': {'precision': 0.9792060491493384, 'recall': 0.9592592592592593, 'f1-score': 0.9691300280636108, 'support': 540}, 'accuracy': 0.9700272479564033, 'macro avg': {'precision': 0.9703722553439, 'recall': 0.9698257080610022, 'f1-score': 0.9700019072356889, 'support': 1101}, 'weighted avg': {'precision': 0.9702037633639597, 'recall': 0.9700272479564033, 'f1-score': 0.9700185370836576, 'support': 1101}}\n"]}],"execution_count":null},{"cell_type":"markdown","source":"### Macro Average Metrics\n\nThe following macro average metrics are derived from the classification report on the test set:\n\n- **Macro Precision**: Measures the average precision across all classes, treating all classes equally.\n- **Macro Recall**: Measures the average recall across all classes, treating all classes equally.\n- **Macro F1-Score**: The average F1-score across all classes, which balances precision and recall.\n\n","metadata":{}},{"cell_type":"code","source":"print(\"Macro Precision:\", test_report[\"macro avg\"][\"precision\"])\nprint(\"Macro Recall:\", test_report[\"macro avg\"][\"recall\"])\nprint(\"Macro F1-score:\", test_report[\"macro avg\"][\"f1-score\"])\n","metadata":{"execution":{"iopub.execute_input":"2025-04-14T14:56:33.009667Z","iopub.status.busy":"2025-04-14T14:56:33.009313Z","iopub.status.idle":"2025-04-14T14:56:33.014155Z","shell.execute_reply":"2025-04-14T14:56:33.013568Z","shell.execute_reply.started":"2025-04-14T14:56:33.009622Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Macro Precision: 0.9703722553439\n","Macro Recall: 0.9698257080610022\n","Macro F1-score: 0.9700019072356889\n"]}],"execution_count":38}]}